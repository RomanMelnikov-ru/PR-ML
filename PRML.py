import streamlit as st
st.set_page_config(layout="wide")
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import uuid
import hashlib
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, RobustScaler, MinMaxScaler
from sklearn.linear_model import LinearRegression, LassoCV
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.metrics import (
    mean_squared_error,
    r2_score,
    mean_absolute_error,
    mean_absolute_percentage_error,
)
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
import joblib
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
import os
from scipy import stats
from scipy.stats import pearsonr, spearmanr, shapiro
import tensorflow as tf
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.inspection import permutation_importance
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import logging
import warnings
import xgboost as xgb

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.get_logger().setLevel('ERROR')
# –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()

# --- –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ XGBoost ---
try:
    from xgboost import XGBRegressor

    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    st.warning("XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: `pip install xgboost`")


# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
@st.cache_resource(show_spinner=False)
def load_data_cached(uploaded_file):
    return pd.read_excel(uploaded_file)


@st.cache_data(show_spinner=False)
def calculate_correlation_matrix(df, method='pearson'):
    return df.corr(method=method)


@st.cache_data(show_spinner=False)
def calculate_vif_cached(X):
    vif_data = pd.DataFrame()
    vif_data["feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
    vif_data["VIF"] = vif_data["VIF"].round(2)
    return vif_data


def load_data():
    st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    st.write("""
    –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ Excel (.xlsx). 
    –§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã:
    - **B, C, D, E, F, G, H**: –ü—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ).
    - **A**: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è.
    """)

    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª Excel", type=["xlsx"])

    if uploaded_file is not None:
        try:
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
                df = load_data_cached(uploaded_file)

            # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            df = df.dropna(axis=1, how='all')

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏
            has_missing = df.isnull().any().any()
            has_inf = np.isinf(df.select_dtypes(include=[np.number])).any().any()

            if has_missing or has_inf:
                st.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–ª–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏. –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
                # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                col1, col2 = st.columns(2)
                with col1:
                    strategy = st.radio(
                        "–ú–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤:",
                        ["–ú–µ–¥–∏–∞–Ω–∞", "–°—Ä–µ–¥–Ω–µ–µ", "–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞", "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏"],
                        index=3,
                        key="missing_values_strategy"
                    )
                with col2:
                    constant_value = None
                    if strategy == "–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞":
                        constant_value = st.number_input("–ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∑–∞–º–µ–Ω—ã", value=0.0, key="fill_constant_value")

                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
                df = handle_missing_values(df, strategy, constant_value)

                st.success(f"–ü—Ä–æ–ø—É—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –º–µ—Ç–æ–¥–æ–º: {strategy}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            if 'A' not in df.columns:
                st.error("–û—à–∏–±–∫–∞: –í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'A' (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)")
                return None

            feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df.columns]
            if not feature_cols:
                st.error("–û—à–∏–±–∫–∞: –í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ (B, C, D, E, F, G –∏–ª–∏ H)")
                return None

            st.success(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(feature_cols)}")
            return df

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
            return None
    return None


def get_scaler_from_name(scaling_method):
    if scaling_method == "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)":
        return StandardScaler()
    elif scaling_method == "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)":
        return MinMaxScaler()
    elif scaling_method == "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)":
        return RobustScaler()
    else:
        return None


def handle_missing_values(df, strategy="–ú–µ–¥–∏–∞–Ω–∞", constant_value=None):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å –≤—ã–±–æ—Ä–æ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏."""
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    # –ó–∞–º–µ–Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–µ–π –Ω–∞ NaN
    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)

    # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    if strategy == "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏":
        df = df.dropna(subset=numeric_cols)
    else:
        if strategy == "–ú–µ–¥–∏–∞–Ω–∞":
            imputer = SimpleImputer(strategy='median')
        elif strategy == "–°—Ä–µ–¥–Ω–µ–µ":
            imputer = SimpleImputer(strategy='mean')
        elif strategy == "–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞":
            imputer = SimpleImputer(strategy='constant', fill_value=constant_value)

        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    selector = VarianceThreshold()
    selector.fit(df[numeric_cols])
    df = df.iloc[:, selector.get_support(indices=True)]

    return df


def handle_outliers_iqr(df, columns=None):
    df_clean = df.copy()
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if columns is not None:
        numeric_cols = [c for c in numeric_cols if c in columns]
    total_outliers = 0
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index
        total_outliers += len(outliers)
        df_clean.loc[outliers, col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)
    if total_outliers > 0:
        st.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR): –∑–∞–º–µ–Ω–µ–Ω–æ {total_outliers} –∑–Ω–∞—á–µ–Ω–∏–π")
    else:
        st.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR): –≤—ã–±—Ä–æ—Å—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
    return df_clean


def calculate_vif(X):
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ X –Ω–µ –ø—É—Å—Ç–æ–π –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        if X.empty or not all(X.dtypes.apply(lambda x: np.issubdtype(x, np.number))):
            return pd.DataFrame({"feature": X.columns, "VIF": [np.nan] * len(X.columns)})

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ VIF
        X_clean = X.dropna()
        if len(X_clean) < 2:
            return pd.DataFrame({"feature": X.columns, "VIF": [np.nan] * len(X.columns)})

        return calculate_vif_cached(X_clean)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ VIF: {str(e)}")
        return pd.DataFrame({"feature": X.columns, "VIF": [np.nan] * len(X.columns)})


def show_descriptive_analysis(df):
    st.subheader("–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    desc = df.describe().T
    desc = desc.rename(columns={
        'count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
        'mean': '–°—Ä–µ–¥–Ω–µ–µ',
        'std': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ',
        'min': '–ú–∏–Ω–∏–º—É–º',
        '25%': '25-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (Q1)',
        '50%': '–ú–µ–¥–∏–∞–Ω–∞ (Q2)',
        '75%': '75-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (Q3)',
        'max': '–ú–∞–∫—Å–∏–º—É–º'
    })
    desc.index.name = '–ü—Ä–∏–∑–Ω–∞–∫'
    st.dataframe(desc.style.format(precision=4))

    st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        fig = px.histogram(
            df,
            x=col,
            nbins=30,
            title=f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ {col}',
            labels={'x': col, 'y': '–ß–∞—Å—Ç–æ—Ç–∞'}
        )
        st.plotly_chart(fig, use_container_width=True)

    st.subheader("–ü–∞—Ä–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è")
    fig = px.scatter_matrix(df[numeric_cols], dimensions=numeric_cols)
    st.plotly_chart(fig, use_container_width=True)


def calculate_correlation_ratio(df, target_col):
    categories = df.drop(target_col, axis=1).columns
    ratios = {}
    for cat in categories:
        grouped = df.groupby(cat)[target_col]
        n = len(df)
        y_mean = df[target_col].mean()
        ss_total = ((df[target_col] - y_mean) ** 2).sum()
        ss_between = grouped.apply(lambda x: len(x) * (x.mean() - y_mean) ** 2).sum()
        eta_squared = ss_between / ss_total
        ratios[cat] = eta_squared
    return ratios


def calculate_pvalues(df, method='pearson'):
    df = df._get_numeric_data()
    cols = df.columns
    n = len(cols)
    p = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    for i, r in enumerate(cols):
        for j, c in enumerate(cols):
            if i == j:
                p.iloc[i, j] = 0
                continue
            x = df[r]
            y = df[c]
            if x.nunique() == 1 or y.nunique() == 1:
                p.iloc[i, j] = np.nan
                continue
            try:
                if method == 'pearson':
                    p.iloc[i, j] = pearsonr(x, y)[1]
                elif method == 'spearman':
                    p.iloc[i, j] = spearmanr(x, y)[1]
            except:
                p.iloc[i, j] = np.nan
    return p


def add_significance_stars(corr_matrix, p_matrix):
    sig_matrix = np.empty_like(corr_matrix, dtype=object)
    for i in range(corr_matrix.shape[0]):
        for j in range(corr_matrix.shape[1]):
            if i == j:
                sig_matrix[i, j] = ""
                continue
            corr = corr_matrix.iloc[i, j]
            p = p_matrix.iloc[i, j]
            if pd.isna(p) or pd.isna(corr):
                sig_matrix[i, j] = "NaN"
                continue
            if p < 0.001:
                sig = f"{corr:.2f}***"
            elif p < 0.01:
                sig = f"{corr:.2f}**"
            elif p < 0.05:
                sig = f"{corr:.2f}*"
            else:
                sig = f"{corr:.2f}"
            sig_matrix[i, j] = sig
    return sig_matrix


def show_model_significance(X, y, model):
    try:
        X_with_const = sm.add_constant(X)
        model_sm = sm.OLS(y, X_with_const).fit()
        st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (ANOVA)")
        st.write(model_sm.summary())
        p_value = model_sm.f_pvalue
        st.write(f"F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {model_sm.fvalue:.4f}, p-value: {p_value:.4f}")
        st.markdown("""
        üìä F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
        - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–æ–º
        - –ù—É–ª–µ–≤–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞: –≤—Å–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã = 0 (–º–æ–¥–µ–ª—å –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞)
        - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç ‚â† 0
        - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
        * –ë–æ–ª—å—à–æ–µ F –∏ –º–∞–ª–æ–µ p-value (<0.05) - –º–æ–¥–µ–ª—å –∑–Ω–∞—á–∏–º–∞
        * F = (–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è) / (–ù–µ–æ–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è)
        - –ß–µ–º –±–æ–ª—å—à–µ F, —Ç–µ–º –ª—É—á—à–µ –º–æ–¥–µ–ª—å –æ–±—ä—è—Å–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ
        """)
        if p_value < 0.05:
            st.success("–ú–æ–¥–µ–ª—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–∞ (p < 0.05)")
        else:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–π (p ‚â• 0.05)")
        return model_sm.pvalues[1:].tolist()
    except Exception as e:
        st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {str(e)}")
        return None


def predict_with_model(model, model_name, X_input, result):
    X_input = X_input.copy()
    scaling_method = result.get("scaling_method", "–ù–µ—Ç")
    scaler = result.get("scaler", None)

    try:
        if model_name != "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å" and scaling_method != "–ù–µ—Ç" and scaler is not None:
            X_scaled = scaler.transform(X_input)
            X_input = pd.DataFrame(X_scaled, columns=X_input.columns)

        if model_name == "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å":
            pred = model.predict(X_input).flatten()[0]
        elif model_name == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
            X_log = np.log(X_input.clip(lower=1e-9))
            X_log = np.nan_to_num(X_log, posinf=0, neginf=0)
            pred = model.predict(X_log)[0]
        elif model_name == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
            pred = np.exp(model.predict(X_input)[0])
        elif model_name == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            X_log = np.log(X_input.clip(lower=1e-9))
            X_log = np.nan_to_num(X_log, posinf=0, neginf=0)
            pred = np.exp(model.predict(X_log)[0])
        else:
            pred = model.predict(X_input)[0]
        return float(pred) if pred is not None else None
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")
        return None


def show_formula(coefficients, intercept, feature_names, regression_type, p_values=None, model_pipeline=None,
                 result=None):
    scaler = None
    sigma = None
    mu = None

    if model_pipeline is not None and hasattr(model_pipeline, 'named_steps'):
        for step_name, step in model_pipeline.named_steps.items():
            if isinstance(step, (StandardScaler, MinMaxScaler, RobustScaler)):
                scaler = step
                break

    if scaler is None and result is not None:
        scaler = result.get("scaler")

    if scaler is not None:
        if isinstance(scaler, StandardScaler):
            sigma = scaler.scale_
            mu = scaler.mean_
        elif isinstance(scaler, MinMaxScaler):
            sigma = scaler.data_max_ - scaler.data_min_
            mu = scaler.data_min_
        elif isinstance(scaler, RobustScaler):
            sigma = scaler.scale_
            mu = scaler.center_

    original_coefs = coefficients.copy()
    original_intercept = intercept

    if sigma is not None and mu is not None:
        try:
            if regression_type in ["–õ–∏–Ω–µ–π–Ω–∞—è", "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è", "Lasso"]:
                original_coefs = coefficients / sigma
                original_intercept = intercept - np.sum(coefficients * mu / sigma)
            elif regression_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
                original_coefs = coefficients / sigma
                original_intercept = intercept - np.sum(coefficients * mu / sigma)
            elif regression_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
                original_coefs = coefficients / sigma
                original_intercept = intercept - np.sum(coefficients * mu / sigma)
            elif regression_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
                original_coefs = coefficients / sigma
                original_intercept = intercept - np.sum(coefficients * mu / sigma)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å—á—ë—Ç–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤: {str(e)}")
            st.caption("–§–æ—Ä–º—É–ª–∞ –ø–æ–∫–∞–∑–∞–Ω–∞ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    formula_parts = [f"{original_intercept:.4f}"]

    for i, (coef, name) in enumerate(zip(original_coefs, feature_names)):
        significance = ""
        if p_values is not None and i < len(p_values):
            if p_values[i] < 0.001:
                significance = "***"
            elif p_values[i] < 0.01:
                significance = "**"
            elif p_values[i] < 0.05:
                significance = "*"

        if regression_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
            term = f"{coef:.4f}{significance}*log({name})"
        elif regression_type in ["–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è"] and "^" in name:
            base, power = name.split("^")
            term = f"{coef:.4f}{significance}*{base}^{power}"
        elif regression_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
            continue
        elif regression_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            continue
        else:
            term = f"{coef:.4f}{significance}*{name}"

        formula_parts.append(term)

    if regression_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
        formula = "A = " + " + ".join(formula_parts)
    elif regression_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
        a0 = np.exp(original_intercept)
        terms = [f"{coef:.4f}{significance}*{name}" for coef, name in zip(
            original_coefs,
            feature_names
        )]
        formula = f"A = {a0:.4f} * exp(" + " + ".join(terms) + ")"
    elif regression_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
        a0 = np.exp(original_intercept)
        terms = [f"{name}^{coef:.4f}{significance}" for coef, name in zip(
            original_coefs,
            feature_names
        )]
        formula = f"A = {a0:.4f} * " + " * ".join(terms)
    else:
        formula = "A = " + " + ".join(formula_parts)

    st.subheader("–§–æ—Ä–º—É–ª–∞ –º–æ–¥–µ–ª–∏ (–≤ –∏—Å—Ö–æ–¥–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ)")
    st.write(formula)

    if p_values is not None:
        st.markdown("""
        **–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏:**
        - \*** p < 0.001 ‚Äî –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
        - \** p < 0.01 ‚Äî –≤—ã—Å–æ–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
        - \* p < 0.05 ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ
        """)


def show_feature_importance(coefficients, feature_names, p_values=None, model=None, X=None, y=None):
    if coefficients is not None:
        importance = np.abs(coefficients)
        if p_values is not None:
            significance = []
            for p in p_values:
                if p < 0.001:
                    significance.append("***")
                elif p < 0.01:
                    significance.append("**")
                elif p < 0.05:
                    significance.append("*")
                else:
                    significance.append("")
            text = [f"{imp:.4f}{sig}" for imp, sig in zip(importance, significance)]
        else:
            text = [f"{imp:.4f}" for imp in importance]
        fig = px.bar(
            x=feature_names,
            y=importance,
            text=text,
            labels={"x": "–§–∞–∫—Ç–æ—Ä—ã", "y": "–í–∞–∂–Ω–æ—Å—Ç—å"},
            title="–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏)"
        )
        fig.update_traces(textposition='outside')
        fig_key = f"feature_importance_{uuid.uuid4()}"
        st.plotly_chart(fig, use_container_width=True, key=fig_key)
    elif model is not None and X is not None and y is not None:
        try:
            if hasattr(model, 'feature_importances_'):
                importance = model.feature_importances_
                fig = px.bar(
                    x=feature_names,
                    y=importance,
                    text=[f"{imp:.4f}" for imp in importance],
                    labels={"x": "–§–∞–∫—Ç–æ—Ä—ã", "y": "–í–∞–∂–Ω–æ—Å—Ç—å"},
                    title="–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å)"
                )
                fig.update_traces(textposition='outside')
                st.plotly_chart(fig, use_container_width=True, key=f"feature_importance_{uuid.uuid4()}")
            else:
                with st.spinner("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (permutation importance)..."):
                    if hasattr(model, 'predict'):
                        result = permutation_importance(model, X, y, n_repeats=10, random_state=42,
                                                        scoring='neg_mean_squared_error')
                        importance = result.importances_mean
                        fig = px.bar(
                            x=feature_names,
                            y=importance,
                            text=[f"{imp:.4f}" for imp in importance],
                            labels={"x": "–§–∞–∫—Ç–æ—Ä—ã", "y": "–í–∞–∂–Ω–æ—Å—Ç—å"},
                            title="–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (permutation importance)"
                        )
                        fig.update_traces(textposition='outside')
                        st.plotly_chart(fig, use_container_width=True, key=f"feature_importance_{uuid.uuid4()}")
                    else:
                        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –º–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥–∞ predict")
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}")


def plot_actual_vs_predicted(y_true, y_pred, model_name):
    try:
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()
        fig = px.scatter(
            x=y_true,
            y=y_pred,
            labels={'x': '–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è', 'y': '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è'},
            trendline='lowess',
            trendline_color_override='green',
            title=f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ({model_name})"
        )
        min_val = min(min(y_true), min(y_pred))
        max_val = max(max(y_true), max(y_pred))
        fig.add_trace(go.Scatter(x=[min_val, max_val], y=[min_val, max_val], mode='lines', name='–ò–¥–µ–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è',
                                 line=dict(color='red', dash='dash')))
        st.plotly_chart(fig, use_container_width=True, key=f"actual_vs_predicted_{uuid.uuid4()}")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}")


def plot_residuals(y_true, y_pred, model_name):
    try:
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()
        residuals = y_true - y_pred
        fig = px.scatter(
            x=y_pred,
            y=residuals,
            labels={'x': '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', 'y': '–û—Å—Ç–∞—Ç–∫–∏'},
            title=f"–ì—Ä–∞—Ñ–∏–∫ –æ—Å—Ç–∞—Ç–∫–æ–≤ ({model_name})"
        )
        fig.add_hline(y=0, line_dash='dash', line_color='red')
        st.plotly_chart(fig, use_container_width=True, key=f"residuals_{uuid.uuid4()}")
        st.subheader("–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤")
        try:
            stat, p = shapiro(residuals)
            st.write(f"–¢–µ—Å—Ç –®–∞–ø–∏—Ä–æ-–£–∏–ª–∫–∞: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ = {stat:.4f}, p-value = {p:.4f}")
            if p > 0.05:
                st.success("–û—Å—Ç–∞—Ç–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ (p > 0.05)")
            else:
                st.warning("–û—Å—Ç–∞—Ç–∫–∏ –Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ (p ‚â§ 0.05)")
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å: {str(e)}")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤: {str(e)}")


def get_poly_features_from_pipeline(pipeline):
    for step_name, step in pipeline.named_steps.items():
        if isinstance(step, PolynomialFeatures):
            return step
    return None


def plot_response_surface(model, X, y, feature_names, regression_type, model_key=""):
    try:
        if len(feature_names) < 2:
            st.warning("–î–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –æ—Ç–∫–ª–∏–∫–∞ –Ω—É–∂–Ω–æ –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 –ø—Ä–∏–∑–Ω–∞–∫–∞")
            return
        st.subheader("–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞")
        poly_transformer = None
        original_features = feature_names.copy()
        if hasattr(model, 'named_steps'):
            poly_transformer = get_poly_features_from_pipeline(model)
            if poly_transformer is not None:
                original_features = poly_transformer.feature_names_in_

        col1, col2 = st.columns(2)
        with col1:
            x_axis = st.selectbox("–û—Å—å X", original_features, index=0, key=f"x_axis_{model_key}_{uuid.uuid4()}")
        with col2:
            y_axis = st.selectbox("–û—Å—å Y", [f for f in original_features if f != x_axis], index=0,
                                  key=f"y_axis_{model_key}_{uuid.uuid4()}")

        fixed_values = {}
        for feature in original_features:
            if feature not in [x_axis, y_axis]:
                if len(X[feature].unique()) > 1:
                    key_suffix = hashlib.md5((feature + model_key).encode()).hexdigest()[:8]
                    key = f"fixed_{feature}_{model_key}_{key_suffix}"
                    fixed_values[feature] = st.slider(
                        f"–ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è '{feature}'",
                        min_value=float(X[feature].min()),
                        max_value=float(X[feature].max()),
                        value=float(X[feature].median()),
                        key=key
                    )
                else:
                    fixed_values[feature] = X[feature].iloc[0]

        X_filtered = X.copy()
        y_filtered = y.copy()

        for feature, fixed_val in fixed_values.items():
            if feature in X_filtered.columns:
                min_val = X_filtered[feature].min()
                max_val = X_filtered[feature].max()
                range_val = max_val - min_val
                tol = 0.1 * range_val if range_val != 0 else 0.1
                mask = (X_filtered[feature] >= fixed_val - tol) & (X_filtered[feature] <= fixed_val + tol)
                X_filtered = X_filtered[mask]
                y_filtered = y_filtered[mask]

        if len(X_filtered) == 0:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –±–ª–∏–∑–∫–∏—Ö –∫ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º. –¢–æ—á–∫–∏ –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è.")
        else:
            st.info(f"–ù–∞ –≥—Ä–∞—Ñ–∏–∫–µ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è **{len(X_filtered)}** —Ç–æ—á–µ–∫, –±–ª–∏–∑–∫–∏—Ö –∫ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º.")

        x_range = np.linspace(X[x_axis].min(), X[x_axis].max(), 20)
        y_range = np.linspace(X[y_axis].min(), X[y_axis].max(), 20)
        xx, yy = np.meshgrid(x_range, y_range)

        predict_data = pd.DataFrame({x_axis: xx.ravel(), y_axis: yy.ravel()})
        for feature, value in fixed_values.items():
            predict_data[feature] = value
        predict_data = predict_data[original_features]

        if regression_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
            predict_data = np.log(predict_data.clip(lower=1e-9))
            predict_data = np.nan_to_num(predict_data, posinf=0, neginf=0)
        elif regression_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            predict_data = np.log(predict_data.clip(lower=1e-9))
            predict_data = np.nan_to_num(predict_data, posinf=0, neginf=0)

        try:
            if regression_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
                zz = np.exp(model.predict(predict_data)).reshape(xx.shape)
            elif regression_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
                zz = np.exp(model.predict(predict_data)).reshape(xx.shape)
            elif regression_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
                zz = model.predict(predict_data).reshape(xx.shape)
            else:
                zz = model.predict(predict_data).reshape(xx.shape)

            fig = go.Figure()

            fig.add_trace(go.Surface(
                x=xx, y=yy, z=zz,
                colorscale='Viridis',
                opacity=0.8,
                name='–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞',
                showscale=True,
                colorbar=dict(title="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ")
            ))

            if len(X_filtered) > 0:
                fig.add_trace(go.Scatter3d(
                    x=X_filtered[x_axis],
                    y=X_filtered[y_axis],
                    z=y_filtered,
                    mode='markers',
                    marker=dict(size=2, color='red', opacity=0.8),
                    name='–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–±–ª–∏–∑–∫–∏–µ)',
                    showlegend=True
                ))

            fig.update_layout(
                title=f'–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞: {x_axis} vs {y_axis} ({regression_type})',
                scene=dict(
                    xaxis_title=x_axis,
                    yaxis_title=y_axis,
                    zaxis_title='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ'
                ),
                width=900,
                height=700,
                margin=dict(l=0, r=0, b=0, t=80),
                legend=dict(
                    x=0.7,
                    y=0.9,
                    bgcolor='rgba(255,255,255,0.8)',
                    bordercolor='gray',
                    borderwidth=1
                )
            )

            st.plotly_chart(fig, use_container_width=True, key=f"response_surface_{uuid.uuid4()}")

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏: {str(e)}")


def save_trained_model(model, model_name):
    try:
        if isinstance(model, Sequential):
            buffer = BytesIO()
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π .h5 —Ñ–∞–π–ª
            temp_h5 = f"{model_name}_model.h5"
            model.save(temp_h5)  # Keras 3: —Ñ–æ—Ä–º–∞—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
            # –ß–∏—Ç–∞–µ–º –≤ –±—É—Ñ–µ—Ä
            with open(temp_h5, "rb") as f:
                buffer.write(f.read())
            buffer.seek(0)
            os.remove(temp_h5)  # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª

            st.download_button(
                label=f"–°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}",
                data=buffer,
                file_name=f"{model_name}_model.h5",
                mime="application/octet-stream",
                key=f"download_{model_name.replace(' ', '_')}_{uuid.uuid4()}"
            )
        else:
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (sklearn) ‚Äî –∫–∞–∫ –±—ã–ª–æ
            buffer = BytesIO()
            joblib.dump(model, buffer)
            buffer.seek(0)
            st.download_button(
                label=f"–°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}",
                data=buffer,
                file_name=f"{model_name}_model.pkl",
                mime="application/octet-stream",
                key=f"download_{model_name.replace(' ', '_')}_{uuid.uuid4()}"
            )
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {model_name}: {str(e)}")


def train_model(reg_type, X_train, y_train, X_test, y_test, feature_cols, positive_mask_train, positive_mask_test,
                scaling_method):
    try:
        model = None
        coefficients = None
        intercept = None
        feature_names = feature_cols.copy()
        original_features = feature_cols.copy()
        cv = KFold(n_splits=5, shuffle=True, random_state=42)

        if scaling_method == "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)":
            scaler = StandardScaler()
        elif scaling_method == "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)":
            scaler = MinMaxScaler()
        elif scaling_method == "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)":
            scaler = RobustScaler()
        else:
            scaler = None

        if reg_type == "–õ–∏–Ω–µ–π–Ω–∞—è":
            steps = [LinearRegression()]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_

        elif reg_type == "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è":
            steps = [PolynomialFeatures(degree=2, include_bias=False)]
            if scaler is not None:
                steps.append(scaler)
            steps.append(LinearRegression())
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_
            poly = model.named_steps['polynomialfeatures']
            feature_names = poly.get_feature_names_out(X_train.columns)

        elif reg_type == "–ö—É–±–∏—á–µ—Å–∫–∞—è":
            steps = [PolynomialFeatures(degree=3, include_bias=False)]
            if scaler is not None:
                steps.append(scaler)
            steps.append(LinearRegression())
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_
            poly = model.named_steps['polynomialfeatures']
            feature_names = poly.get_feature_names_out(X_train.columns)

        elif reg_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
            steps = [LinearRegression()]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            X_log_train = np.log(X_train[positive_mask_train].clip(lower=1e-9))
            X_log_train = np.nan_to_num(X_log_train, posinf=0, neginf=0)
            y_train_pos = y_train[positive_mask_train]
            model.fit(X_log_train, y_train_pos)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_

        elif reg_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
            steps = [LinearRegression()]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            y_log_train = np.log(y_train[positive_mask_train].clip(lower=1e-9))
            y_log_train = np.nan_to_num(y_log_train, posinf=0, neginf=0)
            X_train_pos = X_train[positive_mask_train]
            model.fit(X_train_pos, y_log_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_

        elif reg_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            steps = [LinearRegression()]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            X_log_train = np.log(X_train[positive_mask_train].clip(lower=1e-9))
            X_log_train = np.nan_to_num(X_log_train, posinf=0, neginf=0)
            y_log_train = np.log(y_train[positive_mask_train].clip(lower=1e-9))
            y_log_train = np.nan_to_num(y_log_train, posinf=0, neginf=0)
            model.fit(X_log_train, y_log_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_

        elif reg_type == "Lasso":
            steps = [LassoCV(cv=cv, random_state=42)]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            lasso_step = model.named_steps['lassocv']
            coefficients = lasso_step.coef_
            intercept = lasso_step.intercept_

        elif reg_type == "SVR (–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)":
            param_grid = {
                'svr__C': [0.1, 1, 10],
                'svr__epsilon': [0.01, 0.1, 0.5],
                'svr__kernel': ['rbf', 'linear']
            }
            steps = [SVR()]
            if scaler is not None:
                steps.insert(0, scaler)
            base_model = make_pipeline(*steps)
            model = GridSearchCV(base_model, param_grid, cv=cv, scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "Decision Tree (–†–µ—à–∞—é—â–µ–µ –¥–µ—Ä–µ–≤–æ)":
            param_grid = {
                'max_depth': [None, 3, 5, 10],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            model = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=cv,
                                 scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "Random Forest (–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å)":
            param_grid = {
                'n_estimators': [50, 100],
                'max_depth': [None, 5, 10],
                'min_samples_split': [2, 5]
            }
            model = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=cv,
                                 scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "Gradient Boosting (–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)":
            param_grid = {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5]
            }
            model = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid, cv=cv,
                                 scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "HistGradientBoosting (–ë—ã—Å—Ç—Ä—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)":
            param_grid = {
                'learning_rate': [0.01, 0.1],
                'max_iter': [100, 200],
                'max_depth': [None, 10]
            }
            model = GridSearchCV(HistGradientBoostingRegressor(random_state=42), param_grid, cv=cv,
                                 scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "XGBoost (XGBoost)":
            if not XGBOOST_AVAILABLE:
                return {"error": "XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install xgboost"}
            param_grid = {
                'xgbregressor__n_estimators': [50, 100],
                'xgbregressor__max_depth': [3, 6],
                'xgbregressor__learning_rate': [0.01, 0.1],
                'xgbregressor__subsample': [0.8, 1.0]
            }
            steps = [XGBRegressor(objective='reg:squarederror', random_state=42)]
            if scaler is not None:
                steps.insert(0, scaler)
            base_model = make_pipeline(*steps)
            model = GridSearchCV(base_model, param_grid, cv=cv, scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "Gaussian Processes (–ì–∞—É—Å—Å–æ–≤—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã)":
            kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))
            steps = [GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            coefficients = None
            intercept = None

        elif reg_type == "Neural Network (–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å)":
            scaler = StandardScaler() if scaler is None else scaler
            X_train_scaled = scaler.fit_transform(X_train)
            model = Sequential([
                Dense(64, activation='relu', input_dim=X_train_scaled.shape[1]),
                Dense(32, activation='relu'),
                Dense(1)
            ])
            model.compile(optimizer='adam', loss='mse', metrics=['mae'])
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            model.fit(
                X_train_scaled, y_train,
                epochs=100, batch_size=16, validation_split=0.2,
                callbacks=[early_stopping], verbose=0
            )
            model.scaler = scaler
            coefficients = None
            intercept = None

        else:
            return {"error": f"–¢–∏–ø —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ '{reg_type}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"}

        if reg_type in ["–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è"]:
            X_log_test = np.log(X_test[positive_mask_test].clip(lower=1e-9))
            X_log_test = np.nan_to_num(X_log_test, posinf=0, neginf=0)
            y_pred = model.predict(X_log_test)
            y_test_vals = y_test[positive_mask_test].values
        elif reg_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
            y_pred = np.exp(model.predict(X_test[positive_mask_test]))
            y_test_vals = y_test[positive_mask_test].values
        elif reg_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            X_log_test = np.log(X_test[positive_mask_test].clip(lower=1e-9))
            X_log_test = np.nan_to_num(X_log_test, posinf=0, neginf=0)
            y_pred = np.exp(model.predict(X_log_test))
            y_test_vals = y_test[positive_mask_test].values
        elif reg_type == "Neural Network (–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å)":
            X_test_scaled = model.scaler.transform(X_test)
            y_pred = model.predict(X_test_scaled).flatten()
            y_test_vals = y_test.values
        else:
            y_pred = model.predict(X_test)
            y_test_vals = y_test.values

        y_pred_vals = y_pred.flatten() if hasattr(y_pred, 'flatten') else np.array(y_pred)

        if len(y_test_vals) != len(y_pred_vals):
            raise ValueError(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: y_test={len(y_test_vals)}, y_pred={len(y_pred_vals)}")

        r2 = r2_score(y_test_vals, y_pred_vals)
        color = "üü¢" if r2 > 0.7 else "üü°" if r2 > 0.5 else "üî¥"

        return {
            "model": model,
            "metrics": {
                "r2": r2 if r2 is not None else 0,
                "mse": mean_squared_error(y_test_vals, y_pred_vals) if y_test_vals.size > 0 else 0,
                "rmse": np.sqrt(mean_squared_error(y_test_vals, y_pred_vals)) if y_test_vals.size > 0 else 0,
                "mae": mean_absolute_error(y_test_vals, y_pred_vals) if y_test_vals.size > 0 else 0,
                "mape": mean_absolute_percentage_error(y_test_vals, y_pred_vals) if y_test_vals.size > 0 else 0
            },
            "coefficients": coefficients,
            "intercept": intercept,
            "feature_names": feature_names,
            "original_features": original_features,
            "y_test": y_test_vals,
            "y_pred": y_pred_vals,
            "regression_type": reg_type,
            "color": color,
            "scaling_method": scaling_method
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {reg_type}: {str(e)}")
        return {"error": str(e)}


def run_selected_regressions(df, scaling_method, selected_models):
    results = {}
    feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df.columns]
    if len(feature_cols) == 0:
        st.error("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return results, None, None

    X = df[feature_cols]
    y = df["A"]

    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return results, None, None

    try:
        X_train_np = X_train.to_numpy(dtype=np.float64)
        y_train_np = y_train.to_numpy(dtype=np.float64)
        X_test_np = X_test.to_numpy(dtype=np.float64)
        y_test_np = y_test.to_numpy(dtype=np.float64)
        X_train_np = np.nan_to_num(X_train_np, nan=0.0)
        y_train_np = np.nan_to_num(y_train_np, nan=0.0)
        X_test_np = np.nan_to_num(X_test_np, nan=0.0)
        y_test_np = np.nan_to_num(y_test_np, nan=0.0)
        positive_mask_train = (X_train_np > 0).all(axis=1) & (y_train_np > 0)
        positive_mask_test = (X_test_np > 0).all(axis=1) & (y_test_np > 0)
        positive_data_available = positive_mask_train.any()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        positive_mask_train = np.zeros(len(X_train), dtype=bool)
        positive_mask_test = np.zeros(len(X_test), dtype=bool)
        positive_data_available = False

    regression_types = [
        "–õ–∏–Ω–µ–π–Ω–∞—è", "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è", "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è",
        "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è", "–°—Ç–µ–ø–µ–Ω–Ω–∞—è", "Lasso", "SVR (–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)",
        "Decision Tree (–†–µ—à–∞—é—â–µ–µ –¥–µ—Ä–µ–≤–æ)", "Random Forest (–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å)",
        "Gradient Boosting (–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)", "HistGradientBoosting (–ë—ã—Å—Ç—Ä—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)",
        "XGBoost (XGBoost)", "Gaussian Processes (–ì–∞—É—Å—Å–æ–≤—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã)",
        "Neural Network (–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å)"
    ]

    progress_bar = st.progress(0)
    status_text = st.empty()
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {}
        for i, reg_type in enumerate(regression_types):
            if reg_type not in selected_models:
                continue
            if reg_type in ["–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è", "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è", "–°—Ç–µ–ø–µ–Ω–Ω–∞—è"] and not positive_data_available:
                results[reg_type] = {"error": "–¢—Ä–µ–±—É—é—Ç—Å—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"}
                progress_bar.progress((i + 1) / len(selected_models))
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1} –∏–∑ {len(selected_models)} –º–æ–¥–µ–ª–µ–π")
                continue
            future = executor.submit(
                train_model,
                reg_type, X_train, y_train, X_test, y_test,
                feature_cols, positive_mask_train, positive_mask_test, scaling_method
            )
            futures[future] = reg_type
        for i, future in enumerate(as_completed(futures)):
            reg_type = futures[future]
            try:
                result = future.result()
                results[reg_type] = result
                progress_bar.progress((i + 1) / len(selected_models))
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1} –∏–∑ {len(selected_models)} –º–æ–¥–µ–ª–µ–π")
            except Exception as e:
                results[reg_type] = {"error": str(e)}
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {reg_type}: {str(e)}")

    return results, X_train, y_train


def evaluate_with_cross_validation(model, X, y, model_name):
    try:
        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        scores = cross_val_score(model, X, y, cv=cv, scoring='r2')
        return {
            "model_name": model_name,
            "mean_r2": np.mean(scores),
            "std_r2": np.std(scores),
            "all_scores": scores
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_name}: {str(e)}")
        return None


def show_regression_results(results, X_train, y_train):
    st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ R¬≤")
    comparison_data = []
    for reg_type, res in results.items():
        if "error" not in res:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ None –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≤ comparison_data
            r2 = res["metrics"]["r2"] if res["metrics"]["r2"] is not None else 0
            rmse = res["metrics"]["rmse"] if res["metrics"]["rmse"] is not None else 0
            mae = res["metrics"]["mae"] if res["metrics"]["mae"] is not None else 0
            mape = res["metrics"]["mape"] if res["metrics"]["mape"] is not None else 0

            comparison_data.append({
                "–ú–æ–¥–µ–ª—å": f"{res['color']} {reg_type}",
                "R¬≤": res["metrics"]["r2"],
                "–°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞ (RMSE):": res["metrics"]["rmse"],
                "–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE):": res["metrics"]["mae"],
                "–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAPE):": res["metrics"]["mape"]
            })
    if not comparison_data:
        st.error("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")
        return
    comparison_df = pd.DataFrame(comparison_data).sort_values("R¬≤", ascending=False)

    def style_row(row):
        styles = [''] * len(row)
        r2 = row['R¬≤']
        if r2 > 0.7:
            styles[1] = 'background-color: #4CAF50; color: white'
        elif r2 > 0.5:
            styles[1] = 'background-color: #FFC107; color: black'
        else:
            styles[1] = 'background-color: #F44336; color: white'
        return styles

    styled_df = comparison_df.style.apply(style_row, axis=1).format({
        'R¬≤': '{:.4f}', 'RMSE': '{:.4f}', 'MAE': '{:.4f}', 'MAPE': '{:.2%}'
    })
    st.dataframe(styled_df)
    fig = px.bar(comparison_df, x='–ú–æ–¥–µ–ª—å', y='R¬≤', color='R¬≤',
                 color_continuous_scale=['#F44336', '#FFC107', '#4CAF50'], range_color=[0, 1],
                 title='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ R¬≤')
    st.plotly_chart(fig, use_container_width=True, key=f"models_comparison_{uuid.uuid4()}")
    st.markdown('<div id="top-anchor"></div>', unsafe_allow_html=True)
    st.markdown("""
    <style>
    .top-button {
        position: fixed; bottom: 30px; right: 30px; z-index: 9999; background-color: #4CAF50; color: white;
        padding: 12px 18px; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.2); cursor: pointer;
        font-size: 14px; font-weight: bold; border: none; transition: background-color 0.3s;
    }
    .top-button:hover { background-color: #45a049; }
    </style>
    <a href="#top-anchor" style="text-decoration: none;"><button class="top-button">–ù–∞–≤–µ—Ä—Ö</button></a>
    """, unsafe_allow_html=True)
    st.subheader("–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º")
    for reg_type, res in results.items():
        model_title = f"{res['color']} {reg_type}" if "error" not in res else reg_type
        with st.expander(f"–ú–æ–¥–µ–ª—å: {model_title}", expanded=False):
            if "error" in res:
                st.error(f"–û—à–∏–±–∫–∞: {res['error']}")
                continue
            st.write(f"**R¬≤:** {res['metrics']['r2']:.4f}")
            st.write(f"**–°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞ (RMSE):** {res['metrics']['rmse']:.4f}")
            st.write(f"**–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE):** {res['metrics']['mae']:.4f}")
            st.write(f"**–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAPE):** {res['metrics']['mape']:.2%}")
            linear_models = ["–õ–∏–Ω–µ–π–Ω–∞—è", "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è", "Lasso", "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è", "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è",
                             "–°—Ç–µ–ø–µ–Ω–Ω–∞—è"]
            if reg_type in linear_models and res["coefficients"] is not None:
                try:
                    X_for_p = X_train
                    y_for_p = y_train
                    if reg_type in ["–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è"]:
                        poly = res["model"].named_steps['polynomialfeatures']
                        X_poly = poly.transform(X_train)
                        X_for_p = pd.DataFrame(X_poly, index=X_train.index,
                                               columns=poly.get_feature_names_out(X_train.columns))
                        mask = X_for_p.apply(lambda row: np.isfinite(row).all(), axis=1) & np.isfinite(y_for_p)
                        X_for_p = X_for_p[mask]
                        y_for_p = y_for_p[mask]
                    elif reg_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
                        X_for_p = np.log(X_train.clip(lower=1e-9))
                        X_for_p = np.nan_to_num(X_for_p, posinf=0, neginf=0)
                    elif reg_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
                        y_for_p = np.log(y_train.clip(lower=1e-9))
                        y_for_p = np.nan_to_num(y_for_p, posinf=0, neginf=0)
                    elif reg_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
                        X_for_p = np.log(X_train.clip(lower=1e-9))
                        X_for_p = np.nan_to_num(X_for_p, posinf=0, neginf=0)
                        y_for_p = np.log(y_train.clip(lower=1e-9))
                        y_for_p = np.nan_to_num(y_for_p, posinf=0, neginf=0)
                    mask = np.isfinite(X_for_p).all(axis=1) & np.isfinite(y_for_p)
                    X_for_p = X_for_p[mask]
                    y_for_p = y_for_p[mask]
                    p_values = show_model_significance(X_for_p, y_for_p, res["model"])
                    show_formula(
                        res["coefficients"],
                        res["intercept"],
                        res["feature_names"],
                        reg_type,
                        p_values=p_values,
                        model_pipeline=res["model"],
                        result=res
                    )
                    show_feature_importance(res["coefficients"], res["feature_names"], p_values)
                except Exception as e:
                    st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å: {str(e)}")
            else:
                show_feature_importance(res["coefficients"], res["feature_names"], None, res["model"], X_train, y_train)
            plot_actual_vs_predicted(res["y_test"], res["y_pred"], reg_type)
            plot_residuals(res["y_test"], res["y_pred"], reg_type)
            if len(res["original_features"]) >= 2:
                plot_response_surface(res["model"], X_train, y_train, res["original_features"], res["regression_type"],
                                      model_key=reg_type.replace(" ", "_"))

            st.markdown("---")
            analytical_models = [
                "–õ–∏–Ω–µ–π–Ω–∞—è", "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è",
                "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è", "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è", "–°—Ç–µ–ø–µ–Ω–Ω–∞—è", "Lasso"
            ]

            if reg_type not in analytical_models:
                save_trained_model(res["model"], reg_type)
            else:
                st.info("–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ñ–æ—Ä–º—É–ª–æ–π. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")


def data_preparation(df):
    st.subheader("–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤")
    if df is None:
        return df

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    total_outliers = 0
    outlier_info = []

    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        n_outliers = len(outliers)
        total_outliers += n_outliers
        if n_outliers > 0:
            outlier_info.append(f"- `{col}`: {n_outliers} –≤—ã–±—Ä–æ—Å–æ–≤ (–≥—Ä–∞–Ω–∏—Ü—ã: {lower_bound:.3f} ‚Äì {upper_bound:.3f})")

    if total_outliers > 0:
        st.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ **{total_outliers} –≤—ã–±—Ä–æ—Å–æ–≤** –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö:")
        for line in outlier_info:
            st.write(line)
    else:
        st.success("–í—ã–±—Ä–æ—Å—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã (–ø–æ –º–µ—Ç–æ–¥—É IQR).")

    return df


def main():
    st.title("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ-—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –æ–±—Ä–∞—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state
    if 'df' not in st.session_state:
        st.session_state.df = None
    if 'processed_df' not in st.session_state:
        st.session_state.processed_df = None
    if 'results' not in st.session_state:
        st.session_state.results = None
    if 'vif_remove_list' not in st.session_state:
        st.session_state.vif_remove_list = []
    if 'scaler_recommendation' not in st.session_state:
        st.session_state.scaler_recommendation = {"recommended": "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)"}
    if 'simulator_inputs' not in st.session_state:
        st.session_state.simulator_inputs = {}
    if 'optimization_history' not in st.session_state:
        st.session_state.optimization_history = []
    if 'optimization_result' not in st.session_state:
        st.session_state.optimization_result = None
    if 'monte_carlo_samples' not in st.session_state:
        st.session_state.monte_carlo_samples = None
    if 'monte_carlo_predictions' not in st.session_state:
        st.session_state.monte_carlo_predictions = None
    if 'sensitivity_analysis' not in st.session_state:
        st.session_state.sensitivity_analysis = None
    if 'last_prediction' not in st.session_state:
        st.session_state.last_prediction = None
    if 'selected_models' not in st.session_state:
        st.session_state.selected_models = [
            "–õ–∏–Ω–µ–π–Ω–∞—è", "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è", "Lasso",
            "Random Forest (–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å)", "Gradient Boosting (–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)",
            "Neural Network (–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å)"
        ]

    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö",
        "2. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö",
        "3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
        "4. –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
        "5. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
    ])

    with tab1:
        st.header("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        df = load_data()
        if df is not None:
            st.session_state.df = df.copy()
            st.session_state.processed_df = df.copy()
            st.success(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(df)}, —Å—Ç–æ–ª–±—Ü–æ–≤: {len(df.columns)}")
        else:
            st.session_state.df = None
            st.session_state.processed_df = None

    with tab2:
        st.header("–ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        if st.session_state.df is not None:
            show_descriptive_analysis(st.session_state.df)
            st.session_state.processed_df = data_preparation(st.session_state.df)
            st.info("–î–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑' –ø–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞.")
        else:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö'")

    with tab3:
        st.header("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        if 'df' not in st.session_state or st.session_state.df is None:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö'.")
            return

        df_original = st.session_state.df.copy()
        numeric_cols = df_original.select_dtypes(include=[np.number]).columns
        target_col = 'A'
        if target_col not in df_original.columns:
            st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'A'")
            return

        st.subheader("–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤")
        total_outliers = 0
        outlier_info = []
        for col in numeric_cols:
            Q1 = df_original[col].quantile(0.25)
            Q3 = df_original[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = df_original[(df_original[col] < lower_bound) | (df_original[col] > upper_bound)]
            n_outliers = len(outliers)
            total_outliers += n_outliers
            if n_outliers > 0:
                outlier_info.append(
                    f"- `{col}`: {n_outliers} –≤—ã–±—Ä–æ—Å–æ–≤ (–≥—Ä–∞–Ω–∏—Ü—ã: {lower_bound:.3f} ‚Äì {upper_bound:.3f})")
        if total_outliers > 0:
            st.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ **{total_outliers} –≤—ã–±—Ä–æ—Å–æ–≤** –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö:")
            for line in outlier_info:
                st.write(line)
        else:
            st.success("–í—ã–±—Ä–æ—Å—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã (–ø–æ –º–µ—Ç–æ–¥—É IQR).")

        st.subheader("–í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤")
        outlier_method = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤:",
            ["–ë–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏", "–ó–∞–º–µ–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ (IQR)", "–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏"],
            help="IQR: –º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö (1.5 * IQR)",
            key="outlier_method_radio"
        )

        df_processed = df_original.copy()
        total_modified = 0
        if outlier_method != "–ë–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏":
            for col in numeric_cols:
                Q1 = df_original[col].quantile(0.25)
                Q3 = df_original[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers_mask = (df_original[col] < lower_bound) | (df_original[col] > upper_bound)
                n_outliers = outliers_mask.sum()
                if n_outliers == 0:
                    continue
                if outlier_method == "–ó–∞–º–µ–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ (IQR)":
                    df_processed[col] = df_processed[col].clip(lower=lower_bound, upper=upper_bound)
                    total_modified += n_outliers
                elif outlier_method == "–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏":
                    indices_to_drop = outliers_mask[outliers_mask].index.intersection(df_processed.index)
                    df_processed = df_processed.drop(index=indices_to_drop)
                    total_modified += len(indices_to_drop)
            if total_modified > 0:
                st.info(f"–ú–µ—Ç–æ–¥: **{outlier_method}**. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {total_modified}")
        else:
            st.session_state.processed_df = df_processed.copy()

        st.subheader("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: –ò—Å—Ö–æ–¥–Ω—ã–µ vs –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        with st.expander("üìä –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", expanded=False):
            st.markdown("""
            1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞:
            - –ú–µ—Ä–∞ –ª–∏–Ω–µ–π–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            - –î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π: –æ—Ç -1 (–ø–æ–ª–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å) –¥–æ +1 (–ø–æ–ª–Ω–∞—è –ø—Ä—è–º–∞—è —Å–≤—è–∑—å)
            - –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º
            - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: 
              * 0.9-1.0 - –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è
              * 0.7-0.9 - —Å–∏–ª—å–Ω–∞—è
              * 0.5-0.7 - —É–º–µ—Ä–µ–Ω–Ω–∞—è
              * 0.3-0.5 - —Å–ª–∞–±–∞—è
              * 0-0.3 - –æ—á–µ–Ω—å —Å–ª–∞–±–∞—è/–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            2. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–∞–Ω–∞:
            - –ú–µ—Ä–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ª–∏–Ω–µ–π–Ω–æ–π)
            - –†–∞–Ω–≥–æ–≤—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            - –£—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º
            - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ Œ∑¬≤:
            - –ú–µ—Ä–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            - –î–∏–∞–ø–∞–∑–æ–Ω: 0-1 (1 - –ø–æ–ª–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
            - –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ–ª—é –¥–∏—Å–ø–µ—Ä—Å–∏–∏ Y, –æ–±—ä—è—Å–Ω–µ–Ω–Ω—É—é X
            - –£—á–∏—Ç—ã–≤–∞–µ—Ç –ª—é–±—ã–µ —Ñ–æ—Ä–º—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            4. VIF (–§–∞–∫—Ç–æ—Ä –∏–Ω—Ñ–ª—è—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏):
            - –ú–µ—Ä–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
            - –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞—Å–∫–æ–ª—å–∫–æ –¥–∏—Å–ø–µ—Ä—Å–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ –∏–∑-–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞–º–∏
            - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
              * VIF = 1 - –Ω–µ—Ç –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
              * VIF > 5 - —É–º–µ—Ä–µ–Ω–Ω–∞—è
              * VIF > 10 - —Å–µ—Ä—å–µ–∑–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞

            **–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏:**
            - \*** p < 0.001 ‚Äî –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
            - \** p < 0.01 ‚Äî –≤—ã—Å–æ–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
            - \* p < 0.05 ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ
            ‚ö†Ô∏è –ü–æ—è—Å–Ω–µ–Ω–∏–µ:
            –ó–≤—ë–∑–¥–æ—á–∫–∏ (*) —É –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é.
            –ß–µ–º –º–µ–Ω—å—à–µ ( p )-–∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø—Ä–æ—Ç–∏–≤ –≥–∏–ø–æ—Ç–µ–∑—ã "–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–≤–µ–Ω –Ω—É–ª—é".
            """)

        feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df_original.columns]
        compare_cols = sorted([target_col] + feature_cols)

        def add_stars_to_corr(corr_matrix, p_matrix):
            sig_matrix = corr_matrix.copy().astype(str)
            for i in range(corr_matrix.shape[0]):
                for j in range(corr_matrix.shape[1]):
                    if i == j:
                        sig_matrix.iloc[i, j] = ""
                        continue
                    corr = corr_matrix.iloc[i, j]
                    p = p_matrix.iloc[i, j]
                    if pd.isna(p) or pd.isna(corr):
                        sig_matrix.iloc[i, j] = "NaN"
                        continue
                    if p < 0.001:
                        sig = f"{corr:.2f}***"
                    elif p < 0.01:
                        sig = f"{corr:.2f}**"
                    elif p < 0.05:
                        sig = f"{corr:.2f}*"
                    else:
                        sig = f"{corr:.2f}"
                    sig_matrix.iloc[i, j] = sig
            return sig_matrix

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### üü¶ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ (–∏—Å—Ö–æ–¥–Ω—ã–µ)")
            corr_orig = df_original[compare_cols].corr(method='pearson').round(2)
            p_orig = calculate_pvalues(df_original[compare_cols], method='pearson')
            corr_orig_stars = add_stars_to_corr(corr_orig, p_orig)
            np.fill_diagonal(corr_orig.values, np.nan)
            fig1 = px.imshow(
                corr_orig,
                text_auto=False,
                color_continuous_scale='RdBu',
                zmin=-1, zmax=1,
                title="–ü–∏—Ä—Å–æ–Ω (–¥–æ)",
                width=700, height=600
            )
            fig1.update_traces(text=corr_orig_stars.values, texttemplate="%{text}", textfont={"size": 14})
            st.plotly_chart(fig1, use_container_width=True)

        with col2:
            st.markdown("### üü© –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)")
            corr_proc = df_processed[compare_cols].corr(method='pearson').round(2)
            p_proc = calculate_pvalues(df_processed[compare_cols], method='pearson')
            corr_proc_stars = add_stars_to_corr(corr_proc, p_proc)
            np.fill_diagonal(corr_proc.values, np.nan)
            fig2 = px.imshow(
                corr_proc,
                text_auto=False,
                color_continuous_scale='RdBu',
                zmin=-1, zmax=1,
                title="–ü–∏—Ä—Å–æ–Ω (–ø–æ—Å–ª–µ)",
                width=700, height=600
            )
            fig2.update_traces(text=corr_proc_stars.values, texttemplate="%{text}", textfont={"size": 14})
            st.plotly_chart(fig2, use_container_width=True)

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### üü¶ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–∞–Ω–∞ (–∏—Å—Ö–æ–¥–Ω—ã–µ)")
            corr_orig_s = df_original[compare_cols].corr(method='spearman').round(2)
            p_orig_s = calculate_pvalues(df_original[compare_cols], method='spearman')
            corr_orig_s_stars = add_stars_to_corr(corr_orig_s, p_orig_s)
            np.fill_diagonal(corr_orig_s.values, np.nan)
            fig3 = px.imshow(
                corr_orig_s,
                text_auto=False,
                color_continuous_scale='RdBu',
                zmin=-1, zmax=1,
                title="–°–ø–∏—Ä–º–∞–Ω (–¥–æ)",
                width=700, height=600
            )
            fig3.update_traces(text=corr_orig_s_stars.values, texttemplate="%{text}", textfont={"size": 14})
            st.plotly_chart(fig3, use_container_width=True)

        with col2:
            st.markdown("### üü© –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–∞–Ω–∞ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)")
            corr_proc_s = df_processed[compare_cols].corr(method='spearman').round(2)
            p_proc_s = calculate_pvalues(df_processed[compare_cols], method='spearman')
            corr_proc_s_stars = add_stars_to_corr(corr_proc_s, p_proc_s)
            np.fill_diagonal(corr_proc_s.values, np.nan)
            fig4 = px.imshow(
                corr_proc_s,
                text_auto=False,
                color_continuous_scale='RdBu',
                zmin=-1, zmax=1,
                title="–°–ø–∏—Ä–º–∞–Ω (–ø–æ—Å–ª–µ)",
                width=700, height=600
            )
            fig4.update_traces(text=corr_proc_s_stars.values, texttemplate="%{text}", textfont={"size": 14})
            st.plotly_chart(fig4, use_container_width=True)

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### üîó Œ∑¬≤ (–∏—Å—Ö–æ–¥–Ω—ã–µ)")
            eta_orig = calculate_correlation_ratio(df_original, target_col)
            eta_df_orig = pd.DataFrame.from_dict(eta_orig, orient='index', columns=['Œ∑¬≤']).round(3)
            eta_df_orig = eta_df_orig.loc[feature_cols] if feature_cols else eta_df_orig
            fig7 = px.bar(eta_df_orig, y='Œ∑¬≤', color='Œ∑¬≤', range_color=[0, 1], text='Œ∑¬≤', title="Œ∑¬≤ (–¥–æ)")
            fig7.update_traces(texttemplate='%{text:.3f}', textposition='outside')
            st.plotly_chart(fig7, use_container_width=True)

        with col2:
            st.markdown("### üü¢ Œ∑¬≤ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)")
            eta_proc = calculate_correlation_ratio(df_processed, target_col)
            eta_df_proc = pd.DataFrame.from_dict(eta_proc, orient='index', columns=['Œ∑¬≤']).round(3)
            eta_df_proc = eta_df_proc.loc[feature_cols] if feature_cols else eta_df_proc
            fig8 = px.bar(eta_df_proc, y='Œ∑¬≤', color='Œ∑¬≤', range_color=[0, 1], text='Œ∑¬≤', title="Œ∑¬≤ (–ø–æ—Å–ª–µ)")
            fig8.update_traces(texttemplate='%{text:.3f}', textposition='outside')
            st.plotly_chart(fig8, use_container_width=True)

        col1, col2 = st.columns(2)
        st.subheader("üìä –ê–Ω–∞–ª–∏–∑ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ (VIF)")

        all_features = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df_original.columns]

        if all_features:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if 'vif_excluded_features' not in st.session_state:
                st.session_state.vif_excluded_features = []

            col1, col2 = st.columns(2)

            with col1:
                st.markdown("### üìä –ò—Å—Ö–æ–¥–Ω—ã–π VIF (–≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)")
                vif_orig_all = calculate_vif(df_original[all_features])
                fig5 = px.bar(vif_orig_all, x='feature', y='VIF', color='VIF',
                              color_continuous_scale=['green', 'orange', 'red'], range_color=[0, 20],
                              text='VIF', title="VIF - –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏)")
                fig5.add_hline(y=10, line_dash="dash", line_color="red", annotation_text="VIF=10 - –≤—ã—Å–æ–∫–∞—è")
                fig5.add_hline(y=5, line_dash="dash", line_color="orange", annotation_text="VIF=5 - —É–º–µ—Ä–µ–Ω–Ω–∞—è")
                fig5.update_traces(texttemplate='%{text:.1f}', textposition='outside')
                st.plotly_chart(fig5, use_container_width=True)

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É VIF
                high_vif_count_orig = (vif_orig_all['VIF'] >= 10).sum()
                moderate_vif_count_orig = ((vif_orig_all['VIF'] >= 5) & (vif_orig_all['VIF'] < 10)).sum()

                st.info(f"""
                    **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ VIF:**
                    - –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: **{len(all_features)}**
                    - VIF ‚â• 10 (–≤—ã—Å–æ–∫–∞—è): **{high_vif_count_orig}**
                    - 5 ‚â§ VIF < 10 (—É–º–µ—Ä–µ–Ω–Ω–∞—è): **{moderate_vif_count_orig}**
                    - VIF < 5 (–Ω–∏–∑–∫–∞—è): **{len(all_features) - high_vif_count_orig - moderate_vif_count_orig}**
                    """)

            with col2:
                st.markdown("### üìà VIF —Å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

                # –ú—É–ª—å—Ç–∏—Å–µ–ª–µ–∫—Ç –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                excluded_features = st.multiselect(
                    "–ò—Å–∫–ª—é—á–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ VIF:",
                    options=all_features,
                    default=st.session_state.vif_excluded_features,
                    key="vif_exclude_selector"
                )

                st.session_state.vif_excluded_features = excluded_features

                # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                remaining_features = [f for f in all_features if f not in excluded_features]

                if not remaining_features:
                    st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –º–µ–Ω—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
                    remaining_features = all_features[:min(3, len(all_features))]  # –ú–∏–Ω–∏–º—É–º 3 –ø—Ä–∏–∑–Ω–∞–∫–∞
                    st.info(f"–ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã: {', '.join(remaining_features)}")

                st.write(f"**–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(remaining_features)}):** {', '.join(remaining_features)}")

                if excluded_features:
                    st.write(f"**–ò—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(excluded_features)}):** {', '.join(excluded_features)}")

                # –†–∞—Å—á–µ—Ç VIF –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                vif_remaining = calculate_vif(df_original[remaining_features])

                fig6 = px.bar(vif_remaining, x='feature', y='VIF', color='VIF',
                              color_continuous_scale=['green', 'orange', 'red'], range_color=[0, 20],
                              text='VIF', title=f"VIF –ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è {len(excluded_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                fig6.add_hline(y=10, line_dash="dash", line_color="red", annotation_text="VIF=10 - –≤—ã—Å–æ–∫–∞—è")
                fig6.add_hline(y=5, line_dash="dash", line_color="orange", annotation_text="VIF=5 - —É–º–µ—Ä–µ–Ω–Ω–∞—è")
                fig6.update_traces(texttemplate='%{text:.1f}', textposition='outside')
                st.plotly_chart(fig6, use_container_width=True)

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ VIF –ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                high_vif_count_remaining = (vif_remaining['VIF'] >= 10).sum()
                moderate_vif_count_remaining = ((vif_remaining['VIF'] >= 5) & (vif_remaining['VIF'] < 10)).sum()

                st.info(f"""
                    **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è:**
                    - –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: **{len(remaining_features)}**
                    - VIF ‚â• 10 (–≤—ã—Å–æ–∫–∞—è): **{high_vif_count_remaining}**
                    - 5 ‚â§ VIF < 10 (—É–º–µ—Ä–µ–Ω–Ω–∞—è): **{moderate_vif_count_remaining}**
                    - VIF < 5 (–Ω–∏–∑–∫–∞—è): **{len(remaining_features) - high_vif_count_remaining - moderate_vif_count_remaining}**
                    """)

                # –ê–Ω–∞–ª–∏–∑ —É–ª—É—á—à–µ–Ω–∏—è
                if excluded_features:
                    improvement = high_vif_count_orig - high_vif_count_remaining
                    if improvement > 0:
                        st.success(f"‚úÖ –£–ª—É—á—à–µ–Ω–∏–µ: —É–º–µ–Ω—å—à–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å VIF‚â•10 –Ω–∞ {improvement}")
                    elif high_vif_count_remaining == 0:
                        st.success("‚úÖ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å—é")
                    else:
                        st.warning("‚ö†Ô∏è –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å –≤—Å–µ –µ—â–µ –≤—ã—Å–æ–∫–∞—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–∫–ª—é—á–∏—Ç—å –¥—Ä—É–≥–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")

            # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            st.subheader("üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ VIF")

            col1, col2 = st.columns(2)

            with col1:
                # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                comparison_data = {
                    '–ú–µ—Ç—Ä–∏–∫–∞': ['–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', 'VIF ‚â• 10', '5 ‚â§ VIF < 10', 'VIF < 5'],
                    '–ò—Å—Ö–æ–¥–Ω–æ': [
                        len(all_features),
                        high_vif_count_orig,
                        moderate_vif_count_orig,
                        len(all_features) - high_vif_count_orig - moderate_vif_count_orig
                    ],
                    '–ü–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è': [
                        len(remaining_features),
                        high_vif_count_remaining,
                        moderate_vif_count_remaining,
                        len(remaining_features) - high_vif_count_remaining - moderate_vif_count_remaining
                    ]
                }

                comparison_df = pd.DataFrame(comparison_data)
                st.dataframe(
                    comparison_df.style.format({
                        '–ò—Å—Ö–æ–¥–Ω–æ': '{:.0f}',
                        '–ü–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è': '{:.0f}'
                    }).apply(lambda x: [''] * len(x), axis=1)
                )

            with col2:
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–∏—è
                if excluded_features:
                    fig_improve = go.Figure()

                    fig_improve.add_trace(go.Bar(
                        name='–ò—Å—Ö–æ–¥–Ω–æ',
                        x=['VIF ‚â• 10', '5 ‚â§ VIF < 10', 'VIF < 5'],
                        y=[high_vif_count_orig, moderate_vif_count_orig,
                           len(all_features) - high_vif_count_orig - moderate_vif_count_orig],
                        marker_color='blue'
                    ))

                    fig_improve.add_trace(go.Bar(
                        name='–ü–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è',
                        x=['VIF ‚â• 10', '5 ‚â§ VIF < 10', 'VIF < 5'],
                        y=[high_vif_count_remaining, moderate_vif_count_remaining,
                           len(remaining_features) - high_vif_count_remaining - moderate_vif_count_remaining],
                        marker_color='green'
                    ))

                    fig_improve.update_layout(
                        title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è VIF",
                        barmode='group',
                        showlegend=True
                    )

                    st.plotly_chart(fig_improve, use_container_width=True)

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            st.subheader("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏")

            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏—Å–∫–ª—é—á–∏—Ç—å
            if high_vif_count_remaining > 0:
                high_vif_features = vif_remaining[vif_remaining['VIF'] >= 10]['feature'].tolist()
                st.warning(f"**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç—å:** {', '.join(high_vif_features)}")

                # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                if st.button("üóëÔ∏è –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –∏—Å–∫–ª—é—á–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å VIF‚â•10",
                             key="apply_vif_recommendations"):
                    st.session_state.vif_excluded_features.extend(high_vif_features)
                    st.session_state.vif_excluded_features = list(set(st.session_state.vif_excluded_features))
                    st.success(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –∫ –∏—Å–∫–ª—é—á–µ–Ω–∏—é: {', '.join(high_vif_features)}")
                    st.rerun()

            # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞
            if st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è", key="reset_vif_exclusions"):
                st.session_state.vif_excluded_features = []
                st.success("–ò—Å–∫–ª—é—á–µ–Ω–∏—è —Å–±—Ä–æ—à–µ–Ω—ã")
                st.rerun()

            # –ü–µ—Ä–µ–Ω–æ—Å –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            if excluded_features and st.button("üìã –ü–µ—Ä–µ–Ω–µ—Å—Ç–∏ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å–ø–∏—Å–æ–∫ —É–¥–∞–ª–µ–Ω–∏—è",
                                               key="transfer_to_remove"):
                st.session_state.vif_remove_list.extend(excluded_features)
                st.session_state.vif_remove_list = list(set(st.session_state.vif_remove_list))
                st.success(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {', '.join(excluded_features)}")

        else:
            st.info("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è VIF –∞–Ω–∞–ª–∏–∑–∞.")

    with tab4:
        st.header("–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        if st.session_state.processed_df is not None:
            if "A" not in st.session_state.processed_df.columns:
                st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'A'")
            else:
                df = st.session_state.processed_df.copy()
                feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df.columns]

                st.subheader("–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å—é (VIF ‚â• 10)")
                if len(feature_cols) > 0:
                    X_vif = df[feature_cols]
                    vif_data = calculate_vif(X_vif)
                    high_vif = vif_data[vif_data['VIF'] >= 10]

                    if not high_vif.empty:
                        st.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å—é (VIF ‚â• 10):")
                        for _, row in high_vif.iterrows():
                            checked = st.checkbox(
                                f"–£–¥–∞–ª–∏—Ç—å '{row['feature']}' (VIF = {row['VIF']:.2f})",
                                value=(row['feature'] in st.session_state.vif_remove_list),
                                key=f"vif_remove_{row['feature']}"
                            )
                            if checked and row['feature'] not in st.session_state.vif_remove_list:
                                st.session_state.vif_remove_list.append(row['feature'])
                            elif not checked and row['feature'] in st.session_state.vif_remove_list:
                                st.session_state.vif_remove_list.remove(row['feature'])
                        st.info(f"–ü–æ–º–µ—á–µ–Ω—ã –∫ —É–¥–∞–ª–µ–Ω–∏—é: {', '.join(st.session_state.vif_remove_list)}")
                    else:
                        st.success("–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (VIF < 10 –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
                        st.session_state.vif_remove_list = []
                else:
                    st.warning("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ VIF.")
                    st.session_state.vif_remove_list = []

                st.write("### –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
                st.markdown("""
                - **StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)**: –°—Ä–µ–¥–Ω–µ–µ = 0, std = 1. –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π.
                - **MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)**: –î–∏–∞–ø–∞–∑–æ–Ω [0, 1]. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π.
                - **RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)**: –£—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º.
                - **–ù–µ—Ç**: –ë–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.
                """)
                scaling_method = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è:",
                    ["–ù–µ—Ç", "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)", "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)",
                     "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)"],
                    index=["–ù–µ—Ç", "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)", "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)",
                           "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)"]
                    .index(st.session_state.get('scaler_recommendation', {}).get('recommended', 'StandardScaler')),
                    key="scaling_method_regression"
                )

                # –ù–æ–≤—ã–π –∞–∫–∫–æ—Ä–¥–µ–æ–Ω –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π
                with st.expander("–í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è", expanded=True):
                    st.write("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –∏–∑ –≥—Ä—É–ø–ø (–≥–∞–ª–æ—á–∫–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏):")

                    # –ì—Ä—É–ø–ø—ã –º–æ–¥–µ–ª–µ–π
                    linear_models = {
                        "–õ–∏–Ω–µ–π–Ω–∞—è": "–ü—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è",
                        "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è": "–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è (—Å—Ç–µ–ø–µ–Ω—å 2)",
                        "–ö—É–±–∏—á–µ—Å–∫–∞—è": "–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è (—Å—Ç–µ–ø–µ–Ω—å 3)",
                        "Lasso": "L1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)"
                    }

                    nonlinear_models = {
                        "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è": "–õ–∏–Ω–µ–π–Ω–∞—è –ø–æ –ª–æ–≥–∞—Ä–∏—Ñ–º–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
                        "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è": "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å",
                        "–°—Ç–µ–ø–µ–Ω–Ω–∞—è": "–°—Ç–µ–ø–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å (y = a*x^b)"
                    }

                    tree_models = {
                        "Decision Tree (–†–µ—à–∞—é—â–µ–µ –¥–µ—Ä–µ–≤–æ)": "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π",
                        "Random Forest (–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å)": "–ê–Ω—Å–∞–º–±–ª—å –¥–µ—Ä–µ–≤—å–µ–≤",
                        "Gradient Boosting (–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)": "–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π",
                        "HistGradientBoosting (–ë—ã—Å—Ç—Ä—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è"
                    }

                    other_models = {
                        "SVR (–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)": "SVM –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏",
                        "XGBoost (XGBoost)": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥",
                        "Gaussian Processes (–ì–∞—É—Å—Å–æ–≤—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã)": "–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
                        "Neural Network (–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å)": "–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω"
                    }

                    # –°–æ–∑–¥–∞–µ–º —á–µ–∫–±–æ–∫—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
                    selected = {}

                    st.markdown("#### –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏")
                    for model, desc in linear_models.items():
                        selected[model] = st.checkbox(f"{model}: {desc}",
                                                      value=model in st.session_state.selected_models,
                                                      key=f"linear_{model}")

                    st.markdown("#### –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è")
                    for model, desc in nonlinear_models.items():
                        selected[model] = st.checkbox(f"{model}: {desc}",
                                                      value=model in st.session_state.selected_models,
                                                      key=f"nonlin_{model}")

                    st.markdown("#### –î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏")
                    for model, desc in tree_models.items():
                        selected[model] = st.checkbox(f"{model}: {desc}",
                                                      value=model in st.session_state.selected_models,
                                                      key=f"tree_{model}")

                    st.markdown("#### –î—Ä—É–≥–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã")
                    for model, desc in other_models.items():
                        selected[model] = st.checkbox(f"{model}: {desc}",
                                                      value=model in st.session_state.selected_models,
                                                      key=f"other_{model}")

                    if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π", key="apply_model_selection"):
                        st.session_state.selected_models = [model for model, is_selected in selected.items() if
                                                            is_selected]
                        st.success(f"–í—ã–±—Ä–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(st.session_state.selected_models)}")

                # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Ä–∞—Å—á—ë—Ç–æ–≤ (–æ—Å—Ç–∞—ë—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
                if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏", key="run_regressions"):
                    with st.spinner("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π..."):
                        if st.session_state.vif_remove_list:
                            available_to_remove = [f for f in st.session_state.vif_remove_list if f in df.columns]
                            if available_to_remove:
                                df = df.drop(columns=available_to_remove)
                                st.success(f"–£–¥–∞–ª–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(available_to_remove)}")
                            else:
                                st.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º VIF –Ω–µ –≤—ã–±—Ä–∞–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")
                        else:
                            st.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º VIF –Ω–µ –ø–æ–º–µ—á–µ–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")

                        st.session_state.processed_df = df.copy()

                        results, X_train, y_train = run_selected_regressions(df, scaling_method,
                                                                             st.session_state.selected_models)

                        st.session_state.results = results
                        st.session_state.X_train = X_train
                        st.session_state.y_train = y_train
                        st.rerun()

                if st.session_state.results is not None:
                    show_regression_results(st.session_state.results, st.session_state.X_train,
                                            st.session_state.y_train)
                else:
                    st.info("–ù–∞–∂–º–∏—Ç–µ '–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏', —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏.")

        else:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤–∫–ª–∞–¥–∫–∞—Ö.")

    with tab5:
        st.header("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —Å–∏–º—É–ª—è—Ü–∏—è")
        # –ê–∫–∫–æ—Ä–¥–µ–æ–Ω—ã —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–æ–≤
        with st.expander("üìå –û–±—â–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É", expanded=True):
            st.markdown("""
                **–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è:**  
                –≠—Ç–∞ –≤–∫–ª–∞–¥–∫–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç—Ä–∏ –º–µ—Ç–æ–¥–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏:
                1. **–°–∏–º—É–ª—è—Ç–æ—Ä** ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ¬´—á—Ç–æ –µ—Å–ª–∏¬ª –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–∏–ø–æ—Ç–µ–∑.
                2. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è A (–º–∏–Ω–∏–º—É–º/–º–∞–∫—Å–∏–º—É–º/–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —á–∏—Å–ª–æ).
                3. **Monte Carlo** ‚Äî –º–∞—Å—Å–æ–≤—ã–π —Å–ª—É—á–∞–π–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

                ‚ñ∂ **–ü–æ—Ä—è–¥–æ–∫ —Ä–∞–±–æ—Ç—ã:**  
                - –°–Ω–∞—á–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–∏–º—É–ª—è—Ç–æ—Ä, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ A.  
                - –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω—É–∂–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.  
                - –ù–∞–∫–æ–Ω–µ—Ü, Monte Carlo –ø–æ–∫–∞–∂–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∏—Å–∫–∏.
                """)

        with st.expander("üéÆ –°–∏–º—É–ª—è—Ç–æ—Ä: –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", expanded=False):
            st.markdown("""
                **–ß—Ç–æ —ç—Ç–æ?**  
                –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.

                **–ó–∞—á–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?**  
                - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.  
                - –£–≤–∏–¥–µ—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (¬´—á—Ç–æ –µ—Å–ª–∏ –ø–æ—Å—Ç–∞–≤–∏—Ç—å X=1000?¬ª).  
                - –ë—ã—Å—Ç—Ä–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã.

                **–ü—Ä–∏–º–µ—Ä:**  
                –ï—Å–ª–∏ –¥–≤–∏–≥–∞—Ç—å –ø–æ–ª–∑—É–Ω–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ B, –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ A —Ä–∞—Å—Ç—ë—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ –ø–æ—Å–ª–µ B=50.
                """)

        with st.expander("üîç –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –ø–æ–∏—Å–∫ —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤", expanded=False):
            st.markdown("""
                **–ß—Ç–æ —ç—Ç–æ?**  
                –ê–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞—é—Ç **–º–∏–Ω–∏–º—É–º, –º–∞–∫—Å–∏–º—É–º –∏–ª–∏ —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A**.

                **–ó–∞—á–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?**  
                - –ù–∞–π—Ç–∏ —É—Å–ª–æ–≤–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏ (A ‚Üí max).  
                - –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è (A = 100 ¬± 5).  
                - –û–±–Ω–∞—Ä—É–∂–∏—Ç—å ¬´—É–∑–∫–∏–µ –º–µ—Å—Ç–∞¬ª –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω—ã–π A).

                **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç?**  
                –ú–µ—Ç–æ–¥–æ–º —á–∏—Å–ª–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (SciPy) –∏—â–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â—É—é —É—Å–ª–æ–≤–∏—é.  
                ‚ö†Ô∏è **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:** –ú–æ–∂–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ (–Ω–µ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ) —ç–∫—Å—Ç—Ä–µ–º—É–º—ã!
                """)

        with st.expander("üîÑ Monte Carlo: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", expanded=False):
            st.markdown("""
                **–ß—Ç–æ —ç—Ç–æ?**  
                –ú–∞—Å—Å–æ–≤—ã–π —Å–ª—É—á–∞–π–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –º–æ–¥–µ–ª—å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Ç—ã—Å—è—á–∏ —Ä–∞–∑ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

                **–ó–∞—á–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?**  
                - –£–≤–∏–¥–µ—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π A.  
                - –û—Ü–µ–Ω–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, P(A > 80)).  
                - –ù–∞–π—Ç–∏ ¬´–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ¬ª –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–≥–¥–µ A –≤—Å–µ–≥–¥–∞ –≤ –Ω—É–∂–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ).

                **–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞:**  
                ¬´–ü—Ä–∏ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –≤ 90% —Å–ª—É—á–∞–µ–≤ A ‚àà [40, 60], –º–∞–∫—Å–∏–º—É–º ‚Äî 72.3¬ª.

                **–û—Ç–ª–∏—á–∏–µ –æ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**  
                Monte Carlo –Ω–µ –∏—â–µ—Ç –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç, –∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏—Å—Ö–æ–¥–æ–≤.
                """)

        if st.session_state.results is None or st.session_state.processed_df is None:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑'.")
        else:
            valid_results = {k: v for k, v in st.session_state.results.items() if "error" not in v}
            if not valid_results:
                st.warning("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.")
            else:
                model_options = list(valid_results.keys())
                if 'selected_opt_model' not in st.session_state:
                    st.session_state.selected_opt_model = model_options[0]
                st.session_state.selected_opt_model = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
                    model_options,
                    index=model_options.index(st.session_state.selected_opt_model),
                    key="opt_model_select"
                )
                best_model_name = st.session_state.selected_opt_model
                best_result = valid_results[best_model_name]
                model = best_result["model"]
                feature_cols = best_result["original_features"]
                df = st.session_state.processed_df
                st.success(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: **{best_model_name}** (R¬≤ = {best_result['metrics']['r2']:.4f})")

                # –ù–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                st.subheader("üîç –ü–æ–∏—Å–∫ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("–ù–∞–π—Ç–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ A"):
                        with st.spinner("–ü–æ–∏—Å–∫ –º–∏–Ω–∏–º—É–º–∞..."):
                            try:
                                from scipy.optimize import minimize

                                def objective(x):
                                    X_point = pd.DataFrame([x], columns=feature_cols)
                                    return predict_with_model(model, best_model_name, X_point, best_result)

                                bounds = [(df[col].min(), df[col].max()) for col in feature_cols]
                                initial_guess = [df[col].median() for col in feature_cols]

                                result = minimize(objective, initial_guess, bounds=bounds)

                                if result.success:
                                    st.session_state.opt_result = {
                                        "type": "min",
                                        "value": result.fun,
                                        "params": dict(zip(feature_cols, result.x))
                                    }
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {str(e)}")

                with col2:
                    if st.button("–ù–∞–π—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ A"):
                        with st.spinner("–ü–æ–∏—Å–∫ –º–∞–∫—Å–∏–º—É–º–∞..."):
                            try:
                                from scipy.optimize import minimize

                                def objective(x):
                                    X_point = pd.DataFrame([x], columns=feature_cols)
                                    return -predict_with_model(model, best_model_name, X_point, best_result)

                                bounds = [(df[col].min(), df[col].max()) for col in feature_cols]
                                initial_guess = [df[col].median() for col in feature_cols]

                                result = minimize(objective, initial_guess, bounds=bounds)

                                if result.success:
                                    st.session_state.opt_result = {
                                        "type": "max",
                                        "value": -result.fun,
                                        "params": dict(zip(feature_cols, result.x))
                                    }
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {str(e)}")

                with col3:
                    target_a = st.number_input("–¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A",
                                               value=float(df["A"].mean()),
                                               min_value=float(df["A"].min()),
                                               max_value=float(df["A"].max()))

                    if st.button("–ù–∞–π—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ A"):
                        with st.spinner("–ü–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤..."):
                            try:
                                from scipy.optimize import minimize

                                def objective(x):
                                    X_point = pd.DataFrame([x], columns=feature_cols)
                                    pred = predict_with_model(model, best_model_name, X_point, best_result)
                                    return (pred - target_a) ** 2

                                bounds = [(df[col].min(), df[col].max()) for col in feature_cols]
                                initial_guess = [df[col].median() for col in feature_cols]

                                result = minimize(objective, initial_guess, bounds=bounds)

                                if result.success:
                                    pred_value = predict_with_model(model, best_model_name,
                                                                    pd.DataFrame([result.x], columns=feature_cols),
                                                                    best_result)
                                    st.session_state.opt_result = {
                                        "type": "target",
                                        "target": target_a,
                                        "actual": pred_value,
                                        "params": dict(zip(feature_cols, result.x))
                                    }
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {str(e)}")

                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                if 'opt_result' in st.session_state:
                    res = st.session_state.opt_result
                    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")

                    if res["type"] == "min":
                        st.success(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A: **{res['value']:.4f}**")
                    elif res["type"] == "max":
                        st.success(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A: **{res['value']:.4f}**")
                    elif res["type"] == "target":
                        st.success(f"–¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A: {res['target']:.4f}")
                        st.success(f"–ü–æ–ª—É—á–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A: **{res['actual']:.4f}**")

                    st.write("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
                    params_df = pd.DataFrame.from_dict(res["params"], orient='index', columns=['–ó–Ω–∞—á–µ–Ω–∏–µ'])
                    st.dataframe(params_df.style.format("{:.4f}"))

                    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Å–∏–º—É–ª—è—Ç–æ—Ä–µ
                    if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Å–∏–º—É–ª—è—Ç–æ—Ä–µ"):
                        for col, val in res["params"].items():
                            if col in st.session_state.simulator_inputs:
                                st.session_state.simulator_inputs[col] = val
                        st.rerun()

                st.subheader("üéÆ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å–∏–º—É–ª—è—Ç–æ—Ä")
                st.caption(
                    "–ó–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. "
                    "–≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞."
                )

                cols = st.columns(len(feature_cols))
                inputs = {}
                for i, col in enumerate(feature_cols):
                    with cols[i % len(cols)]:
                        if col not in st.session_state.simulator_inputs:
                            st.session_state.simulator_inputs[col] = float(df[col].median())
                        value = st.number_input(
                            f"{col}",
                            value=st.session_state.simulator_inputs[col],
                            min_value=float(df[col].min()),
                            max_value=float(df[col].max()),
                            step=0.01,
                            help=f"–î–æ–ø—É—Å—Ç–∏–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: –æ—Ç {df[col].min():.3f} –¥–æ {df[col].max():.3f}",
                            key=f"sim_input_{col}"
                        )
                        st.session_state.simulator_inputs[col] = value
                        inputs[col] = value

                if st.button("üßÆ –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å", key="recalculate_prediction"):
                    try:
                        X_input = pd.DataFrame([inputs])
                        pred = predict_with_model(model, best_model_name, X_input, best_result)
                        st.session_state.last_prediction = pred
                        if st.session_state.last_prediction is not None:
                            st.metric("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A", f"{st.session_state.last_prediction:.4f}")
                        else:
                            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ")
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")
                else:
                    if 'last_prediction' in st.session_state and st.session_state.last_prediction is not None:
                        st.metric("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A", f"{st.session_state.last_prediction:.4f}")
                    else:
                        st.write("–ù–∞–∂–º–∏—Ç–µ **–ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å**, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.")

                st.subheader("üîç –ê–Ω–∞–ª–∏–∑ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                st.caption(
                    "–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å = |‚àÇA/‚àÇx_i| ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è A –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞ x_i.\n"
                    "–í—ã—Å–æ–∫–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å = –ø—Ä–∏–∑–Ω–∞–∫ –∫—Ä–∏—Ç–∏—á–µ–Ω. –ù–∏–∑–∫–∞—è = –º–æ–∂–Ω–æ –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–±–æ–¥–Ω–æ."
                )

                if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", key="sensitivity_button"):
                    with st.spinner("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –º–æ–¥–µ–ª–∏..."):
                        try:
                            from scipy.optimize import approx_fprime

                            center = np.array([df[col].median() for col in feature_cols])

                            def predict_func(x):
                                X_point = pd.DataFrame([x], columns=feature_cols)
                                return predict_with_model(model, best_model_name, X_point, best_result)

                            epsilon = 1e-4
                            gradient = approx_fprime(center, predict_func, epsilon)
                            sensitivity = np.abs(gradient)
                            sens_normalized = sensitivity / (sensitivity.max() + 1e-8)

                            threshold = np.median(sensitivity)
                            classification = ["–ñ—ë—Å—Ç–∫–∏–π (–∫—Ä–∏—Ç–∏—á–µ–Ω)" if s > threshold else "–ì–∏–±–∫–∏–π (–≤–∞—Ä—å–∏—Ä—É–µ–º)" for s in
                                              sensitivity]

                            sens_df = pd.DataFrame({
                                "–ü—Ä–∏–∑–Ω–∞–∫": feature_cols,
                                "–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å |‚àÇA/‚àÇx|": sensitivity.round(6),
                                "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è": sens_normalized.round(3),
                                "–¢–∏–ø": classification
                            }).sort_values("–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å |‚àÇA/‚àÇx|", ascending=False)

                            st.session_state.sensitivity_analysis = sens_df

                            st.dataframe(sens_df.style.format({
                                "–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å |‚àÇA/‚àÇx|": "{:.6f}",
                                "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è": "{:.3f}"
                            }).apply(lambda row: [
                                                     'background-color: #ffebee' if row[
                                                                                        '–¢–∏–ø'] == '–ñ—ë—Å—Ç–∫–∏–π (–∫—Ä–∏—Ç–∏—á–µ–Ω)' else 'background-color: #e8f5e9'
                                                 ] * len(row), axis=1))

                            fig_sens = px.bar(
                                sens_df,
                                x="–ü—Ä–∏–∑–Ω–∞–∫",
                                y="–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å |‚àÇA/‚àÇx|",
                                color="–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å |‚àÇA/‚àÇx|",
                                color_continuous_scale=['lightgreen', 'orange', 'red'],
                                text="–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å |‚àÇA/‚àÇx|",
                                title="–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
                            )
                            fig_sens.update_traces(texttemplate='%{text:.6f}', textposition='outside')
                            fig_sens.add_hline(y=threshold, line_dash="dash", line_color="gray",
                                               annotation_text="–ü–æ—Ä–æ–≥")
                            st.plotly_chart(fig_sens, use_container_width=True)

                            rigid = sens_df[sens_df["–¢–∏–ø"] == "–ñ—ë—Å—Ç–∫–∏–π (–∫—Ä–∏—Ç–∏—á–µ–Ω)"]["–ü—Ä–∏–∑–Ω–∞–∫"].tolist()
                            flexible = sens_df[sens_df["–¢–∏–ø"] == "–ì–∏–±–∫–∏–π (–≤–∞—Ä—å–∏—Ä—É–µ–º)"]["–ü—Ä–∏–∑–Ω–∞–∫"].tolist()
                            st.success(f"**–ñ—ë—Å—Ç–∫–∏–µ:** {', '.join(rigid)}")
                            st.info(f"**–ì–∏–±–∫–∏–µ:** {', '.join(flexible)}")

                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á—ë—Ç–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {str(e)}")

                st.markdown("---")

                st.subheader("üîÑ Monte Carlo: –Ω–∞–π—Ç–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                st.write("–í–≤–µ–¥–∏—Ç–µ –¥–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π A:")
                col1, col2 = st.columns(2)
                with col1:
                    target_low = st.number_input("–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ A", value=float(df["A"].quantile(0.25)),
                                                 key="target_low_mc")
                with col2:
                    target_high = st.number_input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ A", value=float(df["A"].quantile(0.75)),
                                                  key="target_high_mc")

                n_samples = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º—É–ª—è—Ü–∏–π", min_value=1000, max_value=50000, value=10000, step=1000)

                if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å Monte Carlo", key="monte_carlo_run"):
                    with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞ —Ç–æ—á–µ–∫..."):
                        try:
                            low_bounds = [df[col].min() for col in feature_cols]
                            high_bounds = [df[col].max() for col in feature_cols]
                            samples = np.random.uniform(
                                low=low_bounds,
                                high=high_bounds,
                                size=(n_samples, len(feature_cols))
                            )
                            X_samples = pd.DataFrame(samples, columns=feature_cols)

                            preds = []
                            for i in range(n_samples):
                                pred = predict_with_model(model, best_model_name, X_samples.iloc[[i]], best_result)
                                preds.append(pred)
                            preds = np.array(preds)

                            mask = (preds >= target_low) & (preds <= target_high)
                            X_valid = X_samples[mask]
                            valid_preds = preds[mask]

                            if len(X_valid) == 0:
                                st.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Ç–æ—á–∫–∏, –≥–¥–µ A ‚àà [{:.3f}, {:.3f}]".format(target_low,
                                                                                                          target_high))
                            else:
                                st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ **{len(X_valid)}** —Ç–æ—á–µ–∫, –≥–¥–µ A –≤ —Ü–µ–ª–µ–≤–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ")

                                ranges = X_valid.agg(['min', 'max']).round(4)
                                st.write("### üîç –î–æ–ø—É—Å—Ç–∏–º—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                                st.dataframe(ranges)

                                st.session_state.monte_carlo_samples = X_valid.copy()
                                st.session_state.monte_carlo_predictions = valid_preds.copy()

                                fig_hist = px.histogram(
                                    valid_preds,
                                    nbins=30,
                                    title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ A —Å—Ä–µ–¥–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Ç–æ—á–µ–∫",
                                    labels={"value": "A", "count": "–ß–∞—Å—Ç–æ—Ç–∞"}
                                )
                                fig_hist.add_vline(x=target_low, line_dash="dash", line_color="red")
                                fig_hist.add_vline(x=target_high, line_dash="dash", line_color="red")
                                st.plotly_chart(fig_hist, use_container_width=True)

                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ Monte Carlo: {str(e)}")

                if st.session_state.monte_carlo_samples is not None and len(st.session_state.monte_carlo_samples) > 0:
                    st.markdown("---")
                    st.subheader("üìä 2D –ü—Ä–æ–µ–∫—Ü–∏—è –¥–æ–ø—É—Å—Ç–∏–º–æ–π –æ–±–ª–∞—Å—Ç–∏")

                    X_valid = st.session_state.monte_carlo_samples
                    valid_preds = st.session_state.monte_carlo_predictions

                    col1, col2 = st.columns(2)
                    with col1:
                        x_axis = st.selectbox("–û—Å—å X", feature_cols, index=0, key="mc_x_axis")
                    with col2:
                        y_axis = st.selectbox("–û—Å—å Y", [f for f in feature_cols if f != x_axis], index=0,
                                              key="mc_y_axis")

                    fig_2d = px.scatter(
                        X_valid,
                        x=x_axis,
                        y=y_axis,
                        color=valid_preds,
                        color_continuous_scale='Viridis',
                        labels={x_axis: x_axis, y_axis: y_axis},
                        title=f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ —Ç–æ—á–∫–∏: {x_axis} vs {y_axis} (A ‚àà [{target_low:.3f}, {target_high:.3f}])",
                        hover_data={X_valid.index.name or "index": X_valid.index}
                    )
                    fig_2d.update_traces(marker=dict(size=6, opacity=0.8))
                    st.plotly_chart(fig_2d, use_container_width=True)

                    if st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ  –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (–ø–æ–ª—É–ø—Ä–æ–∑—Ä–∞—á–Ω–æ)", key="show_original"):
                        mask_original = (df["A"] >= target_low) & (df["A"] <= target_high)
                        fig_2d.add_scatter(
                            x=df.loc[mask_original, x_axis],
                            y=df.loc[mask_original, y_axis],
                            mode='markers',
                            marker=dict(color='gray', size=4, opacity=0.3),
                            name='–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'
                        )
                        st.plotly_chart(fig_2d, use_container_width=True)

                st.markdown("---")


if __name__ == "__main__":
    main()

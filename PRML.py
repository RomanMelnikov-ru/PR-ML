import streamlit as st

st.set_page_config(layout="wide")
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import uuid
import hashlib
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, RobustScaler, MinMaxScaler
from sklearn.linear_model import LinearRegression, LassoCV
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.metrics import (
    mean_squared_error,
    r2_score,
    mean_absolute_error,
    mean_absolute_percentage_error,
)
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
import joblib
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
import os
from scipy import stats
from scipy.stats import pearsonr, spearmanr, shapiro
import tensorflow as tf
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.inspection import permutation_importance
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import logging
import warnings
import xgboost as xgb

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.get_logger().setLevel('ERROR')
# –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()

# --- –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ XGBoost ---
try:
    from xgboost import XGBRegressor

    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    st.warning("XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: `pip install xgboost`")


# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
# –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel-—Ñ–∞–π–ª–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame.
@st.cache_resource(show_spinner=False)
def load_data_cached(uploaded_file):
    return pd.read_excel(uploaded_file)

# –ö—ç—à–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ (pearson/spearman).
@st.cache_data(show_spinner=False)
def calculate_correlation_matrix(df, method='pearson'):
    return df.corr(method=method)

# –ö—ç—à–∏—Ä—É–µ—Ç —Ä–∞—Å—á–µ—Ç VIF (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏–Ω—Ñ–ª—è—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏.
@st.cache_data(show_spinner=False)
def calculate_vif_cached(X):
    vif_data = pd.DataFrame()
    vif_data["feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
    vif_data["VIF"] = vif_data["VIF"].round(2)
    return vif_data

# –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame.
def load_data():
    st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    st.write("""
    –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ Excel (.xlsx). 
    –§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã:
    - **B, C, D, E, F, G, H**: –ü—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ).
    - **A**: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è.
    """)
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª Excel", type=["xlsx"])
    if uploaded_file is not None:
        try:
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
                df = load_data_cached(uploaded_file)
            df = df.dropna(axis=1, how='all')
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
            if df.isnull().values.any() or np.isinf(df.select_dtypes(include=[np.number]).values).any():
                st.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–ª–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏. –ë—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –º–µ–¥–∏–∞–Ω–æ–π.")
                df = handle_missing_values(df)
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π A
            if 'A' not in df.columns:
                st.error("–û—à–∏–±–∫–∞: –í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'A' (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)")
                return None
            # –ü—Ä–∏–∑–Ω–∞–∫–∏: B, C, D, E, F, G, H
            feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df.columns]
            if not feature_cols:
                st.error("–û—à–∏–±–∫–∞: –í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ (B, C, D, E, F, G –∏–ª–∏ H)")
                return None
            st.success(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(feature_cols)}")
            return df
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
            return None
    return None

# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç scaler –ø–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É –∏–º–µ–Ω–∏ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ç.–¥.).
def get_scaler_from_name(scaling_method):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç scaler –ø–æ –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—é."""
    if scaling_method == "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)":
        return StandardScaler()
    elif scaling_method == "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)":
        return MinMaxScaler()
    elif scaling_method == "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)":
        return RobustScaler()
    else:
        return None

# –ó–∞–º–µ–Ω—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏ –º–µ–¥–∏–∞–Ω–æ–π, —É–¥–∞–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π.
def handle_missing_values(df):
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)
    imputer = SimpleImputer(strategy='median')
    df[numeric_cols] = imputer.fit_transform(df[numeric_cols])
    selector = VarianceThreshold()
    selector.fit(df[numeric_cols])
    df = df.iloc[:, selector.get_support(indices=True)]
    return df

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—ã–±—Ä–æ—Å—ã –º–µ—Ç–æ–¥–æ–º IQR: –∑–∞–º–µ–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ [Q1-1.5*IQR, Q3+1.5*IQR] –Ω–∞ –≥—Ä–∞–Ω–∏—Ü—ã.
def handle_outliers_iqr(df, columns=None):
    df_clean = df.copy()
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if columns is not None:
        numeric_cols = [c for c in numeric_cols if c in columns]
    total_outliers = 0
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index
        total_outliers += len(outliers)
        df_clean.loc[outliers, col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)
    if total_outliers > 0:
        st.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR): –∑–∞–º–µ–Ω–µ–Ω–æ {total_outliers} –∑–Ω–∞—á–µ–Ω–∏–π")
    else:
        st.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR): –≤—ã–±—Ä–æ—Å—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
    return df_clean

# –í—ã—á–∏—Å–ª—è–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç VIF –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π).
def calculate_vif(X):
    return calculate_vif_cached(X)

# –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –∏ scatter matrix.
def show_descriptive_analysis(df):
    st.subheader("–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    desc = df.describe().T
    desc = desc.rename(columns={
        'count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
        'mean': '–°—Ä–µ–¥–Ω–µ–µ',
        'std': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ',
        'min': '–ú–∏–Ω–∏–º—É–º',
        '25%': '25-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (Q1)',
        '50%': '–ú–µ–¥–∏–∞–Ω–∞ (Q2)',
        '75%': '75-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (Q3)',
        'max': '–ú–∞–∫—Å–∏–º—É–º'
    })
    desc.index.name = '–ü—Ä–∏–∑–Ω–∞–∫'
    st.dataframe(desc.style.format(precision=4))

    st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        fig = px.histogram(
            df,
            x=col,
            nbins=30,
            title=f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ {col}',
            labels={'x': col, 'y': '–ß–∞—Å—Ç–æ—Ç–∞'}
        )
        st.plotly_chart(fig, use_container_width=True)

    st.subheader("–ü–∞—Ä–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è")
    fig = px.scatter_matrix(df[numeric_cols], dimensions=numeric_cols)
    st.plotly_chart(fig, use_container_width=True)

# –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–ü–∏—Ä—Å–æ–Ω, –°–ø–∏—Ä–º–∞–Ω), Œ∑¬≤ –∏ VIF; –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å.
def show_correlation_analysis(df):
    st.subheader("–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏")
    if df is None or df.empty:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
        return df

    feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df.columns]
    if 'A' not in df.columns:
        st.error("–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'A' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return df

    corr_cols = sorted(['A'] + feature_cols)
    X_corr = df[corr_cols]

    # Pearson
    st.subheader("–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ü–∏—Ä—Å–æ–Ω–∞ (–Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)")
    try:
        pearson_corr = calculate_correlation_matrix(X_corr, 'pearson').round(2)
        np.fill_diagonal(pearson_corr.values, np.nan)
        pearson_p = calculate_pvalues(X_corr, method='pearson')
        pearson_sig = add_significance_stars(pearson_corr, pearson_p)
        fig_pearson = px.imshow(
            pearson_corr,
            text_auto=False,
            color_continuous_scale=px.colors.diverging.RdBu,
            zmin=-1, zmax=1,
            labels=dict(color="–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"),
            title="–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞",
            width=700, height=600
        )
        fig_pearson.update_traces(text=pearson_sig, texttemplate="%{text}", textfont={"size": 14})
        st.plotly_chart(fig_pearson, use_container_width=True, key=f"pearson_corr_{uuid.uuid4()}")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –∫–∞—Ä—Ç—ã –ü–∏—Ä—Å–æ–Ω–∞: {str(e)}")

    # Spearman
    st.subheader("–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –°–ø–∏—Ä–º–∞–Ω–∞ (–Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)")
    try:
        spearman_corr = calculate_correlation_matrix(X_corr, 'spearman').round(2)
        np.fill_diagonal(spearman_corr.values, np.nan)
        spearman_p = calculate_pvalues(X_corr, method='spearman')
        spearman_sig = add_significance_stars(spearman_corr, spearman_p)
        fig_spearman = px.imshow(
            spearman_corr,
            text_auto=False,
            color_continuous_scale=px.colors.diverging.RdBu,
            zmin=-1, zmax=1,
            labels=dict(color="–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"),
            title="–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–∞–Ω–∞",
            width=700, height=600
        )
        fig_spearman.update_traces(text=spearman_sig, texttemplate="%{text}", textfont={"size": 14})
        st.plotly_chart(fig_spearman, use_container_width=True, key=f"spearman_corr_{uuid.uuid4()}")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –∫–∞—Ä—Ç—ã –°–ø–∏—Ä–º–∞–Ω–∞: {str(e)}")

    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ (Œ∑¬≤)
    st.subheader("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ (Œ∑¬≤) –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π A (–Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)")
    try:
        eta_squared = calculate_correlation_ratio(df, "A")
        eta_df = pd.DataFrame.from_dict(eta_squared, orient='index', columns=['Œ∑¬≤']).round(3)
        eta_df = eta_df.loc[feature_cols]
        fig_eta = px.bar(
            eta_df,
            y='Œ∑¬≤',
            color='Œ∑¬≤',
            color_continuous_scale=px.colors.diverging.RdBu,
            range_color=[0, 1],
            labels={'index': '–§–∞–∫—Ç–æ—Ä—ã', 'y': '–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ Œ∑¬≤'},
            text='Œ∑¬≤',
            title="–°–∏–ª–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç A",
            width=700, height=500
        )
        fig_eta.update_traces(texttemplate='%{text:.3f}', textposition='outside', textfont_size=14)
        st.plotly_chart(fig_eta, use_container_width=True, key=f"eta_squared_{uuid.uuid4()}")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ Œ∑¬≤: {str(e)}")

    # VIF ‚Äî –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    X_vif = df[feature_cols]
    if X_vif.empty:
        st.warning("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ VIF")
        return df

    st.subheader("–ê–Ω–∞–ª–∏–∑ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ (VIF) ‚Äî –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    vif_data = calculate_vif(X_vif)
    vif_data = vif_data.set_index('feature').reindex(feature_cols).reset_index()
    fig_vif = px.bar(
        vif_data,
        x='feature',
        y='VIF',
        color='VIF',
        color_continuous_scale=['green', 'orange', 'red'],
        range_color=[0, 20],
        labels={'feature': '–ü—Ä–∏–∑–Ω–∞–∫', 'y': 'VIF –∑–Ω–∞—á–µ–Ω–∏–µ'},
        text='VIF',
        title="VIF ‚Äî –∞–Ω–∞–ª–∏–∑ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏",
        width=700, height=500
    )
    fig_vif.add_hline(y=5, line_dash="dash", line_color="orange", annotation_text="–£–º–µ—Ä–µ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å")
    fig_vif.add_hline(y=10, line_dash="dash", line_color="red", annotation_text="–í—ã—Å–æ–∫–∞—è –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å")
    fig_vif.update_traces(texttemplate='%{text:.2f}', textposition='outside')
    st.plotly_chart(fig_vif, use_container_width=True, key=f"vif_analysis_{uuid.uuid4()}")

    high_vif = vif_data[vif_data["VIF"] >= 10]
    if not high_vif.empty:
        st.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å—é (VIF ‚â• 10):")
        st.dataframe(high_vif)
        st.info(
            "–í—ã –º–æ–∂–µ—Ç–µ —É–¥–∞–ª–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º VIF –Ω–∞ –≤–∫–ª–∞–¥–∫–µ **'–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑'** –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π.")
    else:
        st.success("–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (VIF < 10 –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")

    return df

# –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ Œ∑¬≤ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.
def calculate_correlation_ratio(df, target_col):
    categories = df.drop(target_col, axis=1).columns
    ratios = {}
    for cat in categories:
        grouped = df.groupby(cat)[target_col]
        n = len(df)
        y_mean = df[target_col].mean()
        ss_total = ((df[target_col] - y_mean) ** 2).sum()
        ss_between = grouped.apply(lambda x: len(x) * (x.mean() - y_mean) ** 2).sum()
        eta_squared = ss_between / ss_total
        ratios[cat] = eta_squared
    return ratios

# –í—ã—á–∏—Å–ª—è–µ—Ç p-–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (–ü–∏—Ä—Å–æ–Ω –∏–ª–∏ –°–ø–∏—Ä–º–∞–Ω).
def calculate_pvalues(df, method='pearson'):
    df = df._get_numeric_data()
    cols = df.columns
    n = len(cols)
    p = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    for i, r in enumerate(cols):
        for j, c in enumerate(cols):
            if i == j:
                p.iloc[i, j] = 0
                continue
            x = df[r]
            y = df[c]
            if x.nunique() == 1 or y.nunique() == 1:
                p.iloc[i, j] = np.nan
                continue
            try:
                if method == 'pearson':
                    p.iloc[i, j] = pearsonr(x, y)[1]
                elif method == 'spearman':
                    p.iloc[i, j] = spearmanr(x, y)[1]
            except:
                p.iloc[i, j] = np.nan
    return p

# –î–æ–±–∞–≤–ª—è–µ—Ç –∑–≤–µ–∑–¥–æ—á–∫–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ (***, **, *) –∫ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ p-–∑–Ω–∞—á–µ–Ω–∏–π.
def add_significance_stars(corr_matrix, p_matrix):
    sig_matrix = np.empty_like(corr_matrix, dtype=object)
    for i in range(corr_matrix.shape[0]):
        for j in range(corr_matrix.shape[1]):
            if i == j:
                sig_matrix[i, j] = ""
                continue
            corr = corr_matrix.iloc[i, j]
            p = p_matrix.iloc[i, j]
            if pd.isna(p) or pd.isna(corr):
                sig_matrix[i, j] = "NaN"
                continue
            if p < 0.001:
                sig = f"{corr:.2f}***"
            elif p < 0.01:
                sig = f"{corr:.2f}**"
            elif p < 0.05:
                sig = f"{corr:.2f}*"
            else:
                sig = f"{corr:.2f}"
            sig_matrix[i, j] = sig
    return sig_matrix

# –ü—Ä–æ–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏ (ANOVA) —Å –ø–æ–º–æ—â—å—é statsmodels, –≤—ã–≤–æ–¥–∏—Ç p-–∑–Ω–∞—á–µ–Ω–∏–µ F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
def show_model_significance(X, y, model):
    try:
        X_with_const = sm.add_constant(X)
        model_sm = sm.OLS(y, X_with_const).fit()
        st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (ANOVA)")
        st.write(model_sm.summary())
        p_value = model_sm.f_pvalue
        st.write(f"F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {model_sm.fvalue:.4f}, p-value: {p_value:.4f}")
        st.markdown("""
        üìä F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
        - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–æ–º
        - –ù—É–ª–µ–≤–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞: –≤—Å–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã = 0 (–º–æ–¥–µ–ª—å –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞)
        - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç ‚â† 0
        - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
        * –ë–æ–ª—å—à–æ–µ F –∏ –º–∞–ª–æ–µ p-value (<0.05) - –º–æ–¥–µ–ª—å –∑–Ω–∞—á–∏–º–∞
        * F = (–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è) / (–ù–µ–æ–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è)
        - –ß–µ–º –±–æ–ª—å—à–µ F, —Ç–µ–º –ª—É—á—à–µ –º–æ–¥–µ–ª—å –æ–±—ä—è—Å–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ
        """)
        if p_value < 0.05:
            st.success("–ú–æ–¥–µ–ª—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–∞ (p < 0.05)")
        else:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–π (p ‚â• 0.05)")
        return model_sm.pvalues[1:].tolist()
    except Exception as e:
        st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {str(e)}")
        return None

# –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å —É—á–µ—Ç–æ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π.
def predict_with_model(model, model_name, X_input, result):
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.
    –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç scaler –∏–∑ result, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å.
    """
    X_input = X_input.copy()
    scaling_method = result.get("scaling_method", "–ù–µ—Ç")
    scaler = result.get("scaler", None)

    try:
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫—Ä–æ–º–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, —É –∫–æ—Ç–æ—Ä–æ–π –æ–Ω –≤—Å—Ç—Ä–æ–µ–Ω)
        if model_name != "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å" and scaling_method != "–ù–µ—Ç" and scaler is not None:
            X_scaled = scaler.transform(X_input)
            X_input = pd.DataFrame(X_scaled, columns=X_input.columns)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        if model_name == "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å":
            pred = model.predict(X_input).flatten()[0]
        elif model_name == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
            X_log = np.log(X_input.clip(lower=1e-9))
            X_log = np.nan_to_num(X_log, posinf=0, neginf=0)
            pred = model.predict(X_log)[0]
        elif model_name == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
            pred = np.exp(model.predict(X_input)[0])
        elif model_name == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            X_log = np.log(X_input.clip(lower=1e-9))
            X_log = np.nan_to_num(X_log, posinf=0, neginf=0)
            pred = np.exp(model.predict(X_log)[0])
        else:
            pred = model.predict(X_input)[0]
        return float(pred)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")
        return np.nan

# –§–æ—Ä–º–∏—Ä—É–µ—Ç –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ñ–æ—Ä–º—É–ª—É –º–æ–¥–µ–ª–∏ —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏, —Å —É—á–µ—Ç–æ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏.
def show_formula(coefficients, intercept, feature_names, regression_type, p_values=None, model_pipeline=None):
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = None
    sigma = None
    mu = None

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å–∫–∞–ª–µ—Ä–∞ –∏ –µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if model_pipeline is not None:
        if 'standardscaler' in model_pipeline.named_steps:
            scaler = model_pipeline.named_steps['standardscaler']
            sigma = scaler.scale_
            mu = scaler.mean_
        elif 'minmaxscaler' in model_pipeline.named_steps:
            scaler = model_pipeline.named_steps['minmaxscaler']
            # –î–ª—è MinMaxScaler: X_scaled = (X - X.min()) / (X.max() - X.min())
            sigma = scaler.data_max_ - scaler.data_min_  # –¥–∏–∞–ø–∞–∑–æ–Ω (max - min)
            mu = scaler.data_min_  # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        elif 'robustscaler' in model_pipeline.named_steps:
            scaler = model_pipeline.named_steps['robustscaler']
            # –î–ª—è RobustScaler: X_scaled = (X - median) / IQR
            sigma = scaler.scale_  # IQR (–º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö)
            mu = scaler.center_  # –º–µ–¥–∏–∞–Ω–∞

    # –ü–µ—Ä–µ—Å—á–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π –º–∞—Å—à—Ç–∞–±
    if scaler is not None and sigma is not None and mu is not None:
        try:
            beta_scaled = coefficients
            original_coefs = beta_scaled / sigma
            original_intercept = intercept - np.sum(beta_scaled * mu / sigma)
            coefficients = original_coefs
            intercept = original_intercept
            st.caption("–§–æ—Ä–º—É–ª–∞ –ø–æ–∫–∞–∑–∞–Ω–∞ –≤ **–∏—Å—Ö–æ–¥–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ** –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è)")
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤: {str(e)}")
            st.caption("–§–æ—Ä–º—É–ª–∞ –ø–æ–∫–∞–∑–∞–Ω–∞ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    formula_parts = []
    if regression_type in ["–õ–∏–Ω–µ–π–Ω–∞—è", "Lasso"]:
        formula_parts.append(f"{intercept:.4f}")
        for i, (coef, name) in enumerate(zip(coefficients, feature_names)):
            significance = ""
            if p_values is not None and i < len(p_values):
                if p_values[i] < 0.001:
                    significance = "***"
                elif p_values[i] < 0.01:
                    significance = "**"
                elif p_values[i] < 0.05:
                    significance = "*"
            formula_parts.append(f"{coef:.4f}{significance}*{name}")
        formula = "A = " + " + ".join(formula_parts)
    elif regression_type in ["–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è"]:
        formula_parts.append(f"{intercept:.4f}")
        for i, (coef, name) in enumerate(zip(coefficients, feature_names)):
            if "^" in name:
                base, power = name.split("^")
                term = f"{coef:.4f}*{base}^{power}"
            else:
                term = f"{coef:.4f}*{name}"
            formula_parts.append(term)
        formula = "A = " + " + ".join(formula_parts)
    elif regression_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
        formula_parts.append(f"{intercept:.4f}")
        for i, (coef, name) in enumerate(zip(coefficients, feature_names)):
            significance = ""
            if p_values is not None and i < len(p_values):
                if p_values[i] < 0.001:
                    significance = "***"
                elif p_values[i] < 0.01:
                    significance = "**"
                elif p_values[i] < 0.05:
                    significance = "*"
            formula_parts.append(f"{coef:.4f}{significance}*log({name})")
        formula = "A = " + " + ".join(formula_parts)
    elif regression_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
        a = np.exp(intercept)
        terms = []
        for i, (coef, name) in enumerate(zip(coefficients, feature_names)):
            significance = ""
            if p_values is not None and i < len(p_values):
                if p_values[i] < 0.001:
                    significance = "***"
                elif p_values[i] < 0.01:
                    significance = "**"
                elif p_values[i] < 0.05:
                    significance = "*"
            terms.append(f"{coef:.4f}{significance}*{name}")
        formula = f"A = {a:.4f} * exp(" + " + ".join(terms) + ")"
    elif regression_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
        a = np.exp(intercept)
        terms = []
        for i, (coef, name) in enumerate(zip(coefficients, feature_names)):
            significance = ""
            if p_values is not None and i < len(p_values):
                if p_values[i] < 0.001:
                    significance = "***"
                elif p_values[i] < 0.01:
                    significance = "**"
                elif p_values[i] < 0.05:
                    significance = "*"
            terms.append(f"{name}^{coef:.4f}{significance}")
        formula = f"A = {a:.4f} * " + " * ".join(terms)
    else:
        formula = "–§–æ—Ä–º—É–ª–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"

    st.subheader("–§–æ—Ä–º—É–ª–∞ –º–æ–¥–µ–ª–∏")
    st.write(formula)
    if p_values is not None:
        st.markdown("""
        **–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏:**
        - \*** p < 0.001 ‚Äî –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
        - \** p < 0.01 ‚Äî –≤—ã—Å–æ–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
        - \* p < 0.05 ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ

        ‚ö†Ô∏è –ü–æ—è—Å–Ω–µ–Ω–∏–µ:
        –ó–≤—ë–∑–¥–æ—á–∫–∏ (*) —É –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é.
        –ß–µ–º –º–µ–Ω—å—à–µ ( p )-–∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø—Ä–æ—Ç–∏–≤ –≥–∏–ø–æ—Ç–µ–∑—ã "–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–≤–µ–Ω –Ω—É–ª—é".
        """)

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º –∏–ª–∏ permutation importance).
def show_feature_importance(coefficients, feature_names, p_values=None, model=None, X=None, y=None):
    if coefficients is not None:
        importance = np.abs(coefficients)
        if p_values is not None:
            significance = []
            for p in p_values:
                if p < 0.001:
                    significance.append("***")
                elif p < 0.01:
                    significance.append("**")
                elif p < 0.05:
                    significance.append("*")
                else:
                    significance.append("")
            text = [f"{imp:.4f}{sig}" for imp, sig in zip(importance, significance)]
        else:
            text = [f"{imp:.4f}" for imp in importance]
        fig = px.bar(
            x=feature_names,
            y=importance,
            text=text,
            labels={"x": "–§–∞–∫—Ç–æ—Ä—ã", "y": "–í–∞–∂–Ω–æ—Å—Ç—å"},
            title="–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏)"
        )
        fig.update_traces(textposition='outside')
        fig_key = f"feature_importance_{uuid.uuid4()}"
        st.plotly_chart(fig, use_container_width=True, key=fig_key)
    elif model is not None and X is not None and y is not None:
        try:
            if hasattr(model, 'feature_importances_'):
                importance = model.feature_importances_
                fig = px.bar(
                    x=feature_names,
                    y=importance,
                    text=[f"{imp:.4f}" for imp in importance],
                    labels={"x": "–§–∞–∫—Ç–æ—Ä—ã", "y": "–í–∞–∂–Ω–æ—Å—Ç—å"},
                    title="–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å)"
                )
                fig.update_traces(textposition='outside')
                st.plotly_chart(fig, use_container_width=True, key=f"feature_importance_{uuid.uuid4()}")
            else:
                with st.spinner("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (permutation importance)..."):
                    if hasattr(model, 'predict'):
                        result = permutation_importance(model, X, y, n_repeats=10, random_state=42,
                                                        scoring='neg_mean_squared_error')
                        importance = result.importances_mean
                        fig = px.bar(
                            x=feature_names,
                            y=importance,
                            text=[f"{imp:.4f}" for imp in importance],
                            labels={"x": "–§–∞–∫—Ç–æ—Ä—ã", "y": "–í–∞–∂–Ω–æ—Å—Ç—å"},
                            title="–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (permutation importance)"
                        )
                        fig.update_traces(textposition='outside')
                        st.plotly_chart(fig, use_container_width=True, key=f"feature_importance_{uuid.uuid4()}")
                    else:
                        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –º–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥–∞ predict")
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}")

# –°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫ "—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ" –∑–Ω–∞—á–µ–Ω–∏—è —Å –∏–¥–µ–∞–ª—å–Ω–æ–π –ª–∏–Ω–∏–µ–π.
def plot_actual_vs_predicted(y_true, y_pred, model_name):
    try:
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()
        fig = px.scatter(
            x=y_true,
            y=y_pred,
            labels={'x': '–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è', 'y': '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è'},
            trendline='lowess',
            trendline_color_override='green',
            title=f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ({model_name})"
        )
        min_val = min(min(y_true), min(y_pred))
        max_val = max(max(y_true), max(y_pred))
        fig.add_trace(go.Scatter(x=[min_val, max_val], y=[min_val, max_val], mode='lines', name='–ò–¥–µ–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è',
                                 line=dict(color='red', dash='dash')))
        st.plotly_chart(fig, use_container_width=True, key=f"actual_vs_predicted_{uuid.uuid4()}")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}")

# –°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫ –æ—Å—Ç–∞—Ç–∫–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å —Å –ø–æ–º–æ—â—å—é —Ç–µ—Å—Ç–∞ –®–∞–ø–∏—Ä–æ-–£–∏–ª–∫–∞.
def plot_residuals(y_true, y_pred, model_name):
    try:
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()
        residuals = y_true - y_pred
        fig = px.scatter(
            x=y_pred,
            y=residuals,
            labels={'x': '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', 'y': '–û—Å—Ç–∞—Ç–∫–∏'},
            title=f"–ì—Ä–∞—Ñ–∏–∫ –æ—Å—Ç–∞—Ç–∫–æ–≤ ({model_name})"
        )
        fig.add_hline(y=0, line_dash='dash', line_color='red')
        st.plotly_chart(fig, use_container_width=True, key=f"residuals_{uuid.uuid4()}")
        st.subheader("–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤")
        try:
            stat, p = shapiro(residuals)
            st.write(f"–¢–µ—Å—Ç –®–∞–ø–∏—Ä–æ-–£–∏–ª–∫–∞: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ = {stat:.4f}, p-value = {p:.4f}")
            if p > 0.05:
                st.success("–û—Å—Ç–∞—Ç–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ (p > 0.05)")
            else:
                st.warning("–û—Å—Ç–∞—Ç–∫–∏ –Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ (p ‚â§ 0.05)")
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å: {str(e)}")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤: {str(e)}")

# –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä–µ–∫—Ç PolynomialFeatures –∏–∑ pipeline –º–æ–¥–µ–ª–∏.
def get_poly_features_from_pipeline(pipeline):
    for step_name, step in pipeline.named_steps.items():
        if isinstance(step, PolynomialFeatures):
            return step
    return None

# –°—Ç—Ä–æ–∏—Ç 3D-–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞ –¥–ª—è –¥–≤—É—Ö –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
def plot_response_surface(model, X, y, feature_names, regression_type, model_key=""):
    try:
        if len(feature_names) < 2:
            st.warning("–î–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –æ—Ç–∫–ª–∏–∫–∞ –Ω—É–∂–Ω–æ –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 –ø—Ä–∏–∑–Ω–∞–∫–∞")
            return
        st.subheader("–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞")
        poly_transformer = None
        original_features = feature_names.copy()
        if hasattr(model, 'named_steps'):
            poly_transformer = get_poly_features_from_pipeline(model)
            if poly_transformer is not None:
                original_features = poly_transformer.feature_names_in_

        col1, col2 = st.columns(2)
        with col1:
            x_axis = st.selectbox("–û—Å—å X", original_features, index=0, key=f"x_axis_{model_key}_{uuid.uuid4()}")
        with col2:
            y_axis = st.selectbox("–û—Å—å Y", [f for f in original_features if f != x_axis], index=0,
                                  key=f"y_axis_{model_key}_{uuid.uuid4()}")

        fixed_values = {}
        for feature in original_features:
            if feature not in [x_axis, y_axis]:
                if len(X[feature].unique()) > 1:
                    key_suffix = hashlib.md5((feature + model_key).encode()).hexdigest()[:8]
                    key = f"fixed_{feature}_{model_key}_{key_suffix}"
                    fixed_values[feature] = st.slider(
                        f"–ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è '{feature}'",
                        min_value=float(X[feature].min()),
                        max_value=float(X[feature].max()),
                        value=float(X[feature].median()),
                        key=key
                    )
                else:
                    fixed_values[feature] = X[feature].iloc[0]

        # --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö: –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –±–ª–∏–∑–∫–∏–µ –∫ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º ---
        tolerance = 0.1  # –î–æ–ª—è –æ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞
        X_filtered = X.copy()
        y_filtered = y.copy()

        for feature, fixed_val in fixed_values.items():
            if feature in X_filtered.columns:
                min_val = X_filtered[feature].min()
                max_val = X_filtered[feature].max()
                range_val = max_val - min_val
                tol = tolerance * range_val if range_val != 0 else 0.1
                mask = (X_filtered[feature] >= fixed_val - tol) & (X_filtered[feature] <= fixed_val + tol)
                X_filtered = X_filtered[mask]
                y_filtered = y_filtered[mask]

        if len(X_filtered) == 0:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –±–ª–∏–∑–∫–∏—Ö –∫ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º. –¢–æ—á–∫–∏ –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è.")
        else:
            st.info(f"–ù–∞ –≥—Ä–∞—Ñ–∏–∫–µ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è **{len(X_filtered)}** —Ç–æ—á–µ–∫, –±–ª–∏–∑–∫–∏—Ö –∫ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º.")

        # --- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–µ—Ç–∫–∏ –¥–ª—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ ---
        x_range = np.linspace(X[x_axis].min(), X[x_axis].max(), 20)
        y_range = np.linspace(X[y_axis].min(), X[y_axis].max(), 20)
        xx, yy = np.meshgrid(x_range, y_range)

        predict_data = pd.DataFrame({x_axis: xx.ravel(), y_axis: yy.ravel()})
        for feature, value in fixed_values.items():
            predict_data[feature] = value
        predict_data = predict_data[original_features]

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if regression_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
            predict_data = np.log(predict_data.clip(lower=1e-9))
            predict_data = np.nan_to_num(predict_data, posinf=0, neginf=0)
        elif regression_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            predict_data = np.log(predict_data.clip(lower=1e-9))
            predict_data = np.nan_to_num(predict_data, posinf=0, neginf=0)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        try:
            if regression_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
                zz = np.exp(model.predict(predict_data)).reshape(xx.shape)
            elif regression_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
                zz = np.exp(model.predict(predict_data)).reshape(xx.shape)
            elif regression_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
                zz = model.predict(predict_data).reshape(xx.shape)
            else:
                zz = model.predict(predict_data).reshape(xx.shape)

            # --- 3D –≥—Ä–∞—Ñ–∏–∫ ---
            fig = go.Figure()

            # –ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞
            fig.add_trace(go.Surface(
                x=xx, y=yy, z=zz,
                colorscale='Viridis',
                opacity=0.8,
                name='–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞',
                showscale=True,
                colorbar=dict(title="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ")
            ))

            # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –±–ª–∏–∑–∫–∏–µ)
            if len(X_filtered) > 0:
                fig.add_trace(go.Scatter3d(
                    x=X_filtered[x_axis],
                    y=X_filtered[y_axis],
                    z=y_filtered,
                    mode='markers',
                    marker=dict(size=2, color='red', opacity=0.8),
                    name='–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–±–ª–∏–∑–∫–∏–µ)',
                    showlegend=True
                ))

            fig.update_layout(
                title=f'–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞: {x_axis} vs {y_axis} ({regression_type})',
                scene=dict(
                    xaxis_title=x_axis,
                    yaxis_title=y_axis,
                    zaxis_title='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ'
                ),
                width=900,
                height=700,
                margin=dict(l=0, r=0, b=0, t=80),
                legend=dict(
                    x=0.7,
                    y=0.9,
                    bgcolor='rgba(255,255,255,0.8)',
                    bordercolor='gray',
                    borderwidth=1
                )
            )

            st.plotly_chart(fig, use_container_width=True, key=f"response_surface_{uuid.uuid4()}")

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏: {str(e)}")

# –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–Ω–æ–ø–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞).
def save_trained_model(model, model_name):
    try:
        if isinstance(model, Sequential):
            buffer = BytesIO()
            model.save(buffer, save_format='h5')
            buffer.seek(0)
            st.download_button(
                label=f"–°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}",
                data=buffer,
                file_name=f"{model_name}_model.h5",
                mime="application/octet-stream",
                key=f"download_{model_name.replace(' ', '_')}_{uuid.uuid4()}"
            )
        else:
            buffer = BytesIO()
            joblib.dump(model, buffer)
            buffer.seek(0)
            st.download_button(
                label=f"–°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}",
                data=buffer,
                file_name=f"{model_name}_model.pkl",
                mime="application/octet-stream",
                key=f"download_{model_name.replace(' ', '_')}_{uuid.uuid4()}"
            )
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}")

# –û–±—É—á–∞–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
def train_model(reg_type, X_train, y_train, X_test, y_test, feature_cols, positive_mask_train, positive_mask_test,
                scaling_method):
    try:
        model = None
        coefficients = None
        intercept = None
        feature_names = feature_cols.copy()
        original_features = feature_cols.copy()
        cv = KFold(n_splits=5, shuffle=True, random_state=42)

        # –í—ã–±–æ—Ä —Å–∫–∞–ª–µ—Ä–∞
        if scaling_method == "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)":
            scaler = StandardScaler()
        elif scaling_method == "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)":
            scaler = MinMaxScaler()
        elif scaling_method == "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)":
            scaler = RobustScaler()
        else:
            scaler = None

        if reg_type == "–õ–∏–Ω–µ–π–Ω–∞—è":
            steps = [LinearRegression()]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_

        elif reg_type == "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è":
            steps = [PolynomialFeatures(degree=2, include_bias=False)]
            if scaler is not None:
                steps.append(scaler)
            steps.append(LinearRegression())
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_
            poly = model.named_steps['polynomialfeatures']
            feature_names = poly.get_feature_names_out(X_train.columns)

        elif reg_type == "–ö—É–±–∏—á–µ—Å–∫–∞—è":
            steps = [PolynomialFeatures(degree=3, include_bias=False)]
            if scaler is not None:
                steps.append(scaler)
            steps.append(LinearRegression())
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_
            poly = model.named_steps['polynomialfeatures']
            feature_names = poly.get_feature_names_out(X_train.columns)

        elif reg_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
            steps = [LinearRegression()]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            X_log_train = np.log(X_train[positive_mask_train].clip(lower=1e-9))
            X_log_train = np.nan_to_num(X_log_train, posinf=0, neginf=0)
            y_train_pos = y_train[positive_mask_train]
            model.fit(X_log_train, y_train_pos)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_

        elif reg_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
            steps = [LinearRegression()]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            y_log_train = np.log(y_train[positive_mask_train].clip(lower=1e-9))
            y_log_train = np.nan_to_num(y_log_train, posinf=0, neginf=0)
            X_train_pos = X_train[positive_mask_train]
            model.fit(X_train_pos, y_log_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_

        elif reg_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            steps = [LinearRegression()]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            X_log_train = np.log(X_train[positive_mask_train].clip(lower=1e-9))
            X_log_train = np.nan_to_num(X_log_train, posinf=0, neginf=0)
            y_log_train = np.log(y_train[positive_mask_train].clip(lower=1e-9))
            y_log_train = np.nan_to_num(y_log_train, posinf=0, neginf=0)
            model.fit(X_log_train, y_log_train)
            reg_step = model.named_steps['linearregression']
            coefficients = reg_step.coef_
            intercept = reg_step.intercept_

        elif reg_type == "Lasso":
            steps = [LassoCV(cv=cv, random_state=42)]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            lasso_step = model.named_steps['lassocv']
            coefficients = lasso_step.coef_
            intercept = lasso_step.intercept_

        elif reg_type == "SVR (–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)":
            param_grid = {
                'svr__C': [0.1, 1, 10],
                'svr__epsilon': [0.01, 0.1, 0.5],
                'svr__kernel': ['rbf', 'linear']
            }
            steps = [SVR()]
            if scaler is not None:
                steps.insert(0, scaler)
            base_model = make_pipeline(*steps)
            model = GridSearchCV(base_model, param_grid, cv=cv, scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "Decision Tree (–†–µ—à–∞—é—â–µ–µ –¥–µ—Ä–µ–≤–æ)":
            param_grid = {
                'max_depth': [None, 3, 5, 10],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            model = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=cv,
                                 scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "Random Forest (–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å)":
            param_grid = {
                'n_estimators': [50, 100],
                'max_depth': [None, 5, 10],
                'min_samples_split': [2, 5]
            }
            model = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=cv,
                                 scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "Gradient Boosting (–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)":
            param_grid = {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5]
            }
            model = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid, cv=cv,
                                 scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "HistGradientBoosting (–ë—ã—Å—Ç—Ä—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)":
            param_grid = {
                'learning_rate': [0.01, 0.1],
                'max_iter': [100, 200],
                'max_depth': [None, 10]
            }
            model = GridSearchCV(HistGradientBoostingRegressor(random_state=42), param_grid, cv=cv,
                                 scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "XGBoost (XGBoost)":
            if not XGBOOST_AVAILABLE:
                return {"error": "XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install xgboost"}
            param_grid = {
                'xgbregressor__n_estimators': [50, 100],
                'xgbregressor__max_depth': [3, 6],
                'xgbregressor__learning_rate': [0.01, 0.1],
                'xgbregressor__subsample': [0.8, 1.0]
            }
            steps = [XGBRegressor(objective='reg:squarederror', random_state=42)]
            if scaler is not None:
                steps.insert(0, scaler)
            base_model = make_pipeline(*steps)
            model = GridSearchCV(base_model, param_grid, cv=cv, scoring='neg_mean_squared_error')
            model.fit(X_train, y_train)
            model = model.best_estimator_
            coefficients = None
            intercept = None

        elif reg_type == "Gaussian Processes (–ì–∞—É—Å—Å–æ–≤—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã)":
            kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))
            steps = [GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)]
            if scaler is not None:
                steps.insert(0, scaler)
            model = make_pipeline(*steps)
            model.fit(X_train, y_train)
            coefficients = None
            intercept = None

        elif reg_type == "Neural Network (–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å)":
            scaler = StandardScaler() if scaler is None else scaler
            X_train_scaled = scaler.fit_transform(X_train)
            model = Sequential([
                Dense(64, activation='relu', input_dim=X_train_scaled.shape[1]),
                Dense(32, activation='relu'),
                Dense(1)
            ])
            model.compile(optimizer='adam', loss='mse', metrics=['mae'])
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            model.fit(
                X_train_scaled, y_train,
                epochs=100, batch_size=16, validation_split=0.2,
                callbacks=[early_stopping], verbose=0
            )
            model.scaler = scaler
            coefficients = None
            intercept = None

        else:
            return {"error": f"–¢–∏–ø —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ '{reg_type}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"}

        # --- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ---
        if reg_type in ["–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è"]:
            X_log_test = np.log(X_test[positive_mask_test].clip(lower=1e-9))
            X_log_test = np.nan_to_num(X_log_test, posinf=0, neginf=0)
            y_pred = model.predict(X_log_test)
            y_test_vals = y_test[positive_mask_test].values
        elif reg_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
            y_pred = np.exp(model.predict(X_test[positive_mask_test]))
            y_test_vals = y_test[positive_mask_test].values
        elif reg_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
            X_log_test = np.log(X_test[positive_mask_test].clip(lower=1e-9))
            X_log_test = np.nan_to_num(X_log_test, posinf=0, neginf=0)
            y_pred = np.exp(model.predict(X_log_test))
            y_test_vals = y_test[positive_mask_test].values
        elif reg_type == "Neural Network (–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å)":
            X_test_scaled = model.scaler.transform(X_test)
            y_pred = model.predict(X_test_scaled).flatten()
            y_test_vals = y_test.values
        else:
            y_pred = model.predict(X_test)
            y_test_vals = y_test.values

        y_pred_vals = y_pred.flatten() if hasattr(y_pred, 'flatten') else np.array(y_pred)

        if len(y_test_vals) != len(y_pred_vals):
            raise ValueError(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: y_test={len(y_test_vals)}, y_pred={len(y_pred_vals)}")

        r2 = r2_score(y_test_vals, y_pred_vals)
        color = "üü¢" if r2 > 0.7 else "üü°" if r2 > 0.5 else "üî¥"

        return {
            "model": model,
            "metrics": {
                "r2": r2,
                "mse": mean_squared_error(y_test_vals, y_pred_vals),
                "rmse": np.sqrt(mean_squared_error(y_test_vals, y_pred_vals)),
                "mae": mean_absolute_error(y_test_vals, y_pred_vals),
                "mape": mean_absolute_percentage_error(y_test_vals, y_pred_vals)
            },
            "coefficients": coefficients,
            "intercept": intercept,
            "feature_names": feature_names,
            "original_features": original_features,
            "y_test": y_test_vals,
            "y_pred": y_pred_vals,
            "regression_type": reg_type,
            "color": color
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {reg_type}: {str(e)}")
        return {"error": str(e)}

# –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º.
def run_all_regressions(df, scaling_method):
    results = {}
    feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df.columns]
    if len(feature_cols) == 0:
        st.error("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return results, None, None

    X = df[feature_cols]
    y = df["A"]

    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return results, None, None

    try:
        X_train_np = X_train.to_numpy(dtype=np.float64)
        y_train_np = y_train.to_numpy(dtype=np.float64)
        X_test_np = X_test.to_numpy(dtype=np.float64)
        y_test_np = y_test.to_numpy(dtype=np.float64)
        X_train_np = np.nan_to_num(X_train_np, nan=0.0)
        y_train_np = np.nan_to_num(y_train_np, nan=0.0)
        X_test_np = np.nan_to_num(X_test_np, nan=0.0)
        y_test_np = np.nan_to_num(y_test_np, nan=0.0)
        positive_mask_train = (X_train_np > 0).all(axis=1) & (y_train_np > 0)
        positive_mask_test = (X_test_np > 0).all(axis=1) & (y_test_np > 0)
        positive_data_available = positive_mask_train.any()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        positive_mask_train = np.zeros(len(X_train), dtype=bool)
        positive_mask_test = np.zeros(len(X_test), dtype=bool)
        positive_data_available = False

    regression_types = [
        "–õ–∏–Ω–µ–π–Ω–∞—è", "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è", "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è",
        "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è", "–°—Ç–µ–ø–µ–Ω–Ω–∞—è", "Lasso", "SVR (–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)",
        "Decision Tree (–†–µ—à–∞—é—â–µ–µ –¥–µ—Ä–µ–≤–æ)", "Random Forest (–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å)",
        "Gradient Boosting (–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)", "HistGradientBoosting (–ë—ã—Å—Ç—Ä—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)",
        "XGBoost (XGBoost)", "Gaussian Processes (–ì–∞—É—Å—Å–æ–≤—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã)",
        "Neural Network (–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å)"
    ]

    progress_bar = st.progress(0)
    status_text = st.empty()
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {}
        for i, reg_type in enumerate(regression_types):
            if reg_type in ["–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è", "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è", "–°—Ç–µ–ø–µ–Ω–Ω–∞—è"] and not positive_data_available:
                results[reg_type] = {"error": "–¢—Ä–µ–±—É—é—Ç—Å—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"}
                progress_bar.progress((i + 1) / len(regression_types))
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1} –∏–∑ {len(regression_types)} –º–æ–¥–µ–ª–µ–π")
                continue
            future = executor.submit(
                train_model,
                reg_type, X_train, y_train, X_test, y_test,
                feature_cols, positive_mask_train, positive_mask_test, scaling_method
            )
            futures[future] = reg_type
        for i, future in enumerate(as_completed(futures)):
            reg_type = futures[future]
            try:
                result = future.result()
                results[reg_type] = result
                progress_bar.progress((i + 1) / len(regression_types))
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1} –∏–∑ {len(regression_types)} –º–æ–¥–µ–ª–µ–π")
            except Exception as e:
                results[reg_type] = {"error": str(e)}
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {reg_type}: {str(e)}")

    return results, X_train, y_train

# –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–µ R¬≤.
def evaluate_with_cross_validation(model, X, y, model_name):
    try:
        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        scores = cross_val_score(model, X, y, cv=cv, scoring='r2')
        return {
            "model_name": model_name,
            "mean_r2": np.mean(scores),
            "std_r2": np.std(scores),
            "all_scores": scores
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_name}: {str(e)}")
        return None

# –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
def show_regression_results(results, X_train, y_train):
    st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ R¬≤")
    comparison_data = []
    for reg_type, res in results.items():
        if "error" not in res:
            comparison_data.append({
                "–ú–æ–¥–µ–ª—å": f"{res['color']} {reg_type}",
                "R¬≤": res["metrics"]["r2"],
                "–°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞ (RMSE):": res["metrics"]["rmse"],
                "–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE):": res["metrics"]["mae"],
                "–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAPE):": res["metrics"]["mape"]
            })
    if not comparison_data:
        st.error("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")
        return
    comparison_df = pd.DataFrame(comparison_data).sort_values("R¬≤", ascending=False)

    def style_row(row):
        styles = [''] * len(row)
        r2 = row['R¬≤']
        if r2 > 0.7:
            styles[1] = 'background-color: #4CAF50; color: white'
        elif r2 > 0.5:
            styles[1] = 'background-color: #FFC107; color: black'
        else:
            styles[1] = 'background-color: #F44336; color: white'
        return styles

    styled_df = comparison_df.style.apply(style_row, axis=1).format({
        'R¬≤': '{:.4f}', 'RMSE': '{:.4f}', 'MAE': '{:.4f}', 'MAPE': '{:.2%}'
    })
    st.dataframe(styled_df)
    fig = px.bar(comparison_df, x='–ú–æ–¥–µ–ª—å', y='R¬≤', color='R¬≤',
                 color_continuous_scale=['#F44336', '#FFC107', '#4CAF50'], range_color=[0, 1],
                 title='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ R¬≤')
    st.plotly_chart(fig, use_container_width=True, key=f"models_comparison_{uuid.uuid4()}")
    st.markdown('<div id="top-anchor"></div>', unsafe_allow_html=True)
    st.markdown("""
    <style>
    .top-button {
        position: fixed; bottom: 30px; right: 30px; z-index: 9999; background-color: #4CAF50; color: white;
        padding: 12px 18px; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.2); cursor: pointer;
        font-size: 14px; font-weight: bold; border: none; transition: background-color 0.3s;
    }
    .top-button:hover { background-color: #45a049; }
    </style>
    <a href="#top-anchor" style="text-decoration: none;"><button class="top-button">–ù–∞–≤–µ—Ä—Ö</button></a>
    """, unsafe_allow_html=True)
    st.subheader("–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º")
    for reg_type, res in results.items():
        model_title = f"{res['color']} {reg_type}" if "error" not in res else reg_type
        with st.expander(f"–ú–æ–¥–µ–ª—å: {model_title}", expanded=False):
            if "error" in res:
                st.error(f"–û—à–∏–±–∫–∞: {res['error']}")
                continue
            st.write(f"**R¬≤:** {res['metrics']['r2']:.4f}")
            st.write(f"**–°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞ (RMSE):** {res['metrics']['rmse']:.4f}")
            st.write(f"**–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE):** {res['metrics']['mae']:.4f}")
            st.write(f"**–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAPE):** {res['metrics']['mape']:.2%}")
            linear_models = ["–õ–∏–Ω–µ–π–Ω–∞—è", "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è", "Lasso", "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è", "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è",
                             "–°—Ç–µ–ø–µ–Ω–Ω–∞—è"]
            if reg_type in linear_models and res["coefficients"] is not None:
                try:
                    X_for_p = X_train
                    y_for_p = y_train
                    if reg_type in ["–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è"]:
                        poly = res["model"].named_steps['polynomialfeatures']
                        X_poly = poly.transform(X_train)
                        X_for_p = pd.DataFrame(X_poly, index=X_train.index,
                                               columns=poly.get_feature_names_out(X_train.columns))
                        mask = X_for_p.apply(lambda row: np.isfinite(row).all(), axis=1) & np.isfinite(y_for_p)
                        X_for_p = X_for_p[mask]
                        y_for_p = y_for_p[mask]
                    elif reg_type == "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è":
                        X_for_p = np.log(X_train.clip(lower=1e-9))
                        X_for_p = np.nan_to_num(X_for_p, posinf=0, neginf=0)
                    elif reg_type == "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è":
                        y_for_p = np.log(y_train.clip(lower=1e-9))
                        y_for_p = np.nan_to_num(y_for_p, posinf=0, neginf=0)
                    elif reg_type == "–°—Ç–µ–ø–µ–Ω–Ω–∞—è":
                        X_for_p = np.log(X_train.clip(lower=1e-9))
                        X_for_p = np.nan_to_num(X_for_p, posinf=0, neginf=0)
                        y_for_p = np.log(y_train.clip(lower=1e-9))
                        y_for_p = np.nan_to_num(y_for_p, posinf=0, neginf=0)
                    mask = np.isfinite(X_for_p).all(axis=1) & np.isfinite(y_for_p)
                    X_for_p = X_for_p[mask]
                    y_for_p = y_for_p[mask]
                    p_values = show_model_significance(X_for_p, y_for_p, res["model"])
                    show_formula(res["coefficients"], res["intercept"], res["feature_names"], reg_type, p_values,
                                 model_pipeline=res["model"])
                    show_feature_importance(res["coefficients"], res["feature_names"], p_values)
                except Exception as e:
                    st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å: {str(e)}")
            else:
                show_feature_importance(res["coefficients"], res["feature_names"], None, res["model"], X_train, y_train)
            plot_actual_vs_predicted(res["y_test"], res["y_pred"], reg_type)
            plot_residuals(res["y_test"], res["y_pred"], reg_type)
            if len(res["original_features"]) >= 2:
                plot_response_surface(res["model"], X_train, y_train, res["original_features"], res["regression_type"],
                                      model_key=reg_type.replace(" ", "_"))

            # --- –ö–Ω–æ–ø–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è ML-–º–æ–¥–µ–ª–µ–π) ---
            st.markdown("---")
            # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ (–Ω–µ ML)
            analytical_models = [
                "–õ–∏–Ω–µ–π–Ω–∞—è", "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è", "–ö—É–±–∏—á–µ—Å–∫–∞—è",
                "–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è", "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è", "–°—Ç–µ–ø–µ–Ω–Ω–∞—è", "Lasso"
            ]

            if reg_type not in analytical_models:
                save_trained_model(res["model"], reg_type)
            else:
                st.info("–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ñ–æ—Ä–º—É–ª–æ–π. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")

# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—ã–±—Ä–æ—Å—ã –≤ –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–µ—Ç–æ–¥—É IQR (—Ç–æ–ª—å–∫–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞).
def data_preparation(df):
    st.subheader("–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤")
    if df is None:
        return df

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    total_outliers = 0
    outlier_info = []

    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        n_outliers = len(outliers)
        total_outliers += n_outliers
        if n_outliers > 0:
            outlier_info.append(f"- `{col}`: {n_outliers} –≤—ã–±—Ä–æ—Å–æ–≤ (–≥—Ä–∞–Ω–∏—Ü—ã: {lower_bound:.3f} ‚Äì {upper_bound:.3f})")

    if total_outliers > 0:
        st.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ **{total_outliers} –≤—ã–±—Ä–æ—Å–æ–≤** –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö:")
        for line in outlier_info:
            st.write(line)
        # üî¥ –£–ë–†–ê–ù–û: st.info("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—ã–±—Ä–æ—Å—ã –≤—Ä—É—á–Ω—É—é –∏–ª–∏ –≤—ã–±—Ä–∞—Ç—å –º–µ—Ç–æ–¥ –Ω–∏–∂–µ.")
    else:
        st.success("–í—ã–±—Ä–æ—Å—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã (–ø–æ –º–µ—Ç–æ–¥—É IQR).")

    return df

# –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ø–∞–Ω–µ–ª—å —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏.
def show_status_panel():
    st.sidebar.header("üìä –°—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞")

    def status_icon(condition):
        return "‚úÖ" if condition else "üü°"

    st.sidebar.write(f"{status_icon(st.session_state.status['data_loaded'])} **–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã**")
    st.sidebar.write(f"{status_icon(st.session_state.status['outliers_detected'])} **–í—ã–±—Ä–æ—Å—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã**")
    st.sidebar.write(f"{status_icon(st.session_state.status['outliers_handled'])} **–í—ã–±—Ä–æ—Å—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã**")
    st.sidebar.write(f"{status_icon(st.session_state.status['vif_analyzed'])} **VIF –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω**")
    st.sidebar.write(f"{status_icon(st.session_state.status['models_trained'])} **–ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã**")

    # –ü—Ä–æ–≥—Ä–µ—Å—Å
    progress = sum(st.session_state.status.values()) / len(st.session_state.status)
    st.sidebar.progress(progress)
    st.sidebar.caption(f"–ì–æ—Ç–æ–≤–æ: {int(progress * 100)}%")

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Streamlit.
def main():
    st.title("–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
    if 'df' not in st.session_state:
        st.session_state.df = None
    if 'processed_df' not in st.session_state:
        st.session_state.processed_df = None
    if 'results' not in st.session_state:
        st.session_state.results = None

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π
    if 'status' not in st.session_state:
        st.session_state.status = {
            'data_loaded': False,
            'outliers_detected': False,
            'outliers_handled': False,
            'vif_analyzed': False,
            'models_trained': False
        }
    show_status_panel()

    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö",
        "2. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö",
        "3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
        "4. –ê–Ω–∞–ª–∏–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è",  # <-- –ù–û–í–´–ô –¢–ê–ë
        "5. –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
        "6. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
    ])

    with tab1:
        st.header("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        df = load_data()
        if df is not None:
            st.session_state.df = df.copy()
            st.session_state.processed_df = df.copy()
            st.session_state.status['data_loaded'] = True  # <-- –î–û–ë–ê–í–õ–ï–ù–û
            st.success(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(df)}, —Å—Ç–æ–ª–±—Ü–æ–≤: {len(df.columns)}")
        else:
            st.session_state.df = None
            st.session_state.processed_df = None
            st.session_state.status['data_loaded'] = False  # –Ø–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ

    with tab2:
        st.header("–ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        if st.session_state.df is not None:
            show_descriptive_analysis(st.session_state.df)
            st.session_state.processed_df = data_preparation(st.session_state.df)
            # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–¥–µ—Å—å ‚Äî –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—â—ë –Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞
            st.info("–î–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑' –ø–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞.")
        else:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö'")

    with tab3:
        st.header("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        # üîí –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ª–∏ –¥–∞–Ω–Ω—ã–µ?
        if 'df' not in st.session_state or st.session_state.df is None:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö'.")
            st.session_state.status['outliers_detected'] = False
            st.session_state.status['outliers_handled'] = False
            st.session_state.status['vif_analyzed'] = False
            return
        # –í—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ–º —Å –∫–æ–ø–∏–µ–π –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df_original = st.session_state.df.copy()
        numeric_cols = df_original.select_dtypes(include=[np.number]).columns
        target_col = 'A'
        if target_col not in df_original.columns:
            st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'A'")
            return

        # --- –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞) ---
        st.subheader("–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤")
        total_outliers = 0
        outlier_info = []
        for col in numeric_cols:
            Q1 = df_original[col].quantile(0.25)
            Q3 = df_original[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = df_original[(df_original[col] < lower_bound) | (df_original[col] > upper_bound)]
            n_outliers = len(outliers)
            total_outliers += n_outliers
            if n_outliers > 0:
                outlier_info.append(
                    f"- `{col}`: {n_outliers} –≤—ã–±—Ä–æ—Å–æ–≤ (–≥—Ä–∞–Ω–∏—Ü—ã: {lower_bound:.3f} ‚Äì {upper_bound:.3f})")
        if total_outliers > 0:
            st.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ **{total_outliers} –≤—ã–±—Ä–æ—Å–æ–≤** –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö:")
            for line in outlier_info:
                st.write(line)
            st.session_state.status['outliers_detected'] = True
        else:
            st.success("–í—ã–±—Ä–æ—Å—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã (–ø–æ –º–µ—Ç–æ–¥—É IQR).")
            st.session_state.status['outliers_detected'] = True
            st.session_state.status['outliers_handled'] = True

        # --- –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ---
        st.subheader("–í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤")
        outlier_method = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤:",
            ["–ë–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏", "–ó–∞–º–µ–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ (IQR)", "–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏"],
            help="IQR: –º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö (1.5 * IQR)",
            key="outlier_method_radio"
        )

        df_processed = df_original.copy()
        total_modified = 0
        if outlier_method != "–ë–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏":
            for col in numeric_cols:
                Q1 = df_original[col].quantile(0.25)
                Q3 = df_original[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers_mask = (df_original[col] < lower_bound) | (df_original[col] > upper_bound)
                n_outliers = outliers_mask.sum()
                if n_outliers == 0:
                    continue
                if outlier_method == "–ó–∞–º–µ–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ (IQR)":
                    df_processed[col] = df_processed[col].clip(lower=lower_bound, upper=upper_bound)
                    total_modified += n_outliers
                elif outlier_method == "–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏":
                    indices_to_drop = outliers_mask[outliers_mask].index.intersection(df_processed.index)
                    df_processed = df_processed.drop(index=indices_to_drop)
                    total_modified += len(indices_to_drop)
            if total_modified > 0:
                st.info(f"–ú–µ—Ç–æ–¥: **{outlier_method}**. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {total_modified}")
            st.session_state.status['outliers_handled'] = True
        else:
            st.session_state.status['outliers_handled'] = False

        st.session_state.processed_df = df_processed.copy()

        # --- –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –¥–æ –∏ –ø–æ—Å–ª–µ ---
        st.subheader("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: –ò—Å—Ö–æ–¥–Ω—ã–µ vs –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        with st.expander("üìä –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", expanded=False):
            st.markdown("""
            1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞:
            - –ú–µ—Ä–∞ –ª–∏–Ω–µ–π–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            - –î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π: –æ—Ç -1 (–ø–æ–ª–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å) –¥–æ +1 (–ø–æ–ª–Ω–∞—è –ø—Ä—è–º–∞—è —Å–≤—è–∑—å)
            - –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º
            - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: 
              * 0.9-1.0 - –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è
              * 0.7-0.9 - —Å–∏–ª—å–Ω–∞—è
              * 0.5-0.7 - —É–º–µ—Ä–µ–Ω–Ω–∞—è
              * 0.3-0.5 - —Å–ª–∞–±–∞—è
              * 0-0.3 - –æ—á–µ–Ω—å —Å–ª–∞–±–∞—è/–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            2. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–∞–Ω–∞:
            - –ú–µ—Ä–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ª–∏–Ω–µ–π–Ω–æ–π)
            - –†–∞–Ω–≥–æ–≤—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            - –£—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º
            - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ Œ∑¬≤:
            - –ú–µ—Ä–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            - –î–∏–∞–ø–∞–∑–æ–Ω: 0-1 (1 - –ø–æ–ª–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
            - –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ–ª—é –¥–∏—Å–ø–µ—Ä—Å–∏–∏ Y, –æ–±—ä—è—Å–Ω–µ–Ω–Ω—É—é X
            - –£—á–∏—Ç—ã–≤–∞–µ—Ç –ª—é–±—ã–µ —Ñ–æ—Ä–º—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            4. VIF (–§–∞–∫—Ç–æ—Ä –∏–Ω—Ñ–ª—è—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏):
            - –ú–µ—Ä–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
            - –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞—Å–∫–æ–ª—å–∫–æ –¥–∏—Å–ø–µ—Ä—Å–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ –∏–∑-–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞–º–∏
            - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
              * VIF = 1 - –Ω–µ—Ç –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
              * VIF > 5 - —É–º–µ—Ä–µ–Ω–Ω–∞—è
              * VIF > 10 - —Å–µ—Ä—å–µ–∑–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞
            
            **–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏:**
            - \*** p < 0.001 ‚Äî –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
            - \** p < 0.01 ‚Äî –≤—ã—Å–æ–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
            - \* p < 0.05 ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ
            ‚ö†Ô∏è –ü–æ—è—Å–Ω–µ–Ω–∏–µ:
            –ó–≤—ë–∑–¥–æ—á–∫–∏ (*) —É –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é.
            –ß–µ–º –º–µ–Ω—å—à–µ ( p )-–∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø—Ä–æ—Ç–∏–≤ –≥–∏–ø–æ—Ç–µ–∑—ã "–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–≤–µ–Ω –Ω—É–ª—é".
            """)

        feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df_original.columns]
        compare_cols = sorted([target_col] + feature_cols)

        # --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–≤—ë–∑–¥–æ—á–µ–∫ ---
        def add_stars_to_corr(corr_matrix, p_matrix):
            sig_matrix = corr_matrix.copy().astype(str)
            for i in range(corr_matrix.shape[0]):
                for j in range(corr_matrix.shape[1]):
                    if i == j:
                        sig_matrix.iloc[i, j] = ""
                        continue
                    corr = corr_matrix.iloc[i, j]
                    p = p_matrix.iloc[i, j]
                    if pd.isna(p) or pd.isna(corr):
                        sig_matrix.iloc[i, j] = "NaN"
                        continue
                    if p < 0.001:
                        sig = f"{corr:.2f}***"
                    elif p < 0.01:
                        sig = f"{corr:.2f}**"
                    elif p < 0.05:
                        sig = f"{corr:.2f}*"
                    else:
                        sig = f"{corr:.2f}"
                    sig_matrix.iloc[i, j] = sig
            return sig_matrix

        # --- –ü–∏—Ä—Å–æ–Ω ---
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### üü¶ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ (–∏—Å—Ö–æ–¥–Ω—ã–µ)")
            corr_orig = df_original[compare_cols].corr(method='pearson').round(2)
            p_orig = calculate_pvalues(df_original[compare_cols], method='pearson')
            corr_orig_stars = add_stars_to_corr(corr_orig, p_orig)
            np.fill_diagonal(corr_orig.values, np.nan)
            fig1 = px.imshow(
                corr_orig,
                text_auto=False,
                color_continuous_scale='RdBu',
                zmin=-1, zmax=1,
                title="–ü–∏—Ä—Å–æ–Ω (–¥–æ)",
                width=700, height=600
            )
            fig1.update_traces(text=corr_orig_stars.values, texttemplate="%{text}", textfont={"size": 14})
            st.plotly_chart(fig1, use_container_width=True)

        with col2:
            st.markdown("### üü© –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)")
            corr_proc = df_processed[compare_cols].corr(method='pearson').round(2)
            p_proc = calculate_pvalues(df_processed[compare_cols], method='pearson')
            corr_proc_stars = add_stars_to_corr(corr_proc, p_proc)
            np.fill_diagonal(corr_proc.values, np.nan)
            fig2 = px.imshow(
                corr_proc,
                text_auto=False,
                color_continuous_scale='RdBu',
                zmin=-1, zmax=1,
                title="–ü–∏—Ä—Å–æ–Ω (–ø–æ—Å–ª–µ)",
                width=700, height=600
            )
            fig2.update_traces(text=corr_proc_stars.values, texttemplate="%{text}", textfont={"size": 14})
            st.plotly_chart(fig2, use_container_width=True)

        # --- –°–ø–∏—Ä–º–∞–Ω ---
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### üü¶ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–∞–Ω–∞ (–∏—Å—Ö–æ–¥–Ω—ã–µ)")
            corr_orig_s = df_original[compare_cols].corr(method='spearman').round(2)
            p_orig_s = calculate_pvalues(df_original[compare_cols], method='spearman')
            corr_orig_s_stars = add_stars_to_corr(corr_orig_s, p_orig_s)
            np.fill_diagonal(corr_orig_s.values, np.nan)
            fig3 = px.imshow(
                corr_orig_s,
                text_auto=False,
                color_continuous_scale='RdBu',
                zmin=-1, zmax=1,
                title="–°–ø–∏—Ä–º–∞–Ω (–¥–æ)",
                width=700, height=600
            )
            fig3.update_traces(text=corr_orig_s_stars.values, texttemplate="%{text}", textfont={"size": 14})
            st.plotly_chart(fig3, use_container_width=True)

        with col2:
            st.markdown("### üü© –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–∞–Ω–∞ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)")
            corr_proc_s = df_processed[compare_cols].corr(method='spearman').round(2)
            p_proc_s = calculate_pvalues(df_processed[compare_cols], method='spearman')
            corr_proc_s_stars = add_stars_to_corr(corr_proc_s, p_proc_s)
            np.fill_diagonal(corr_proc_s.values, np.nan)
            fig4 = px.imshow(
                corr_proc_s,
                text_auto=False,
                color_continuous_scale='RdBu',
                zmin=-1, zmax=1,
                title="–°–ø–∏—Ä–º–∞–Ω (–ø–æ—Å–ª–µ)",
                width=700, height=600
            )
            fig4.update_traces(text=corr_proc_s_stars.values, texttemplate="%{text}", textfont={"size": 14})
            st.plotly_chart(fig4, use_container_width=True)

        # --- Œ∑¬≤ ---
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### üîó Œ∑¬≤ (–∏—Å—Ö–æ–¥–Ω—ã–µ)")
            eta_orig = calculate_correlation_ratio(df_original, target_col)
            eta_df_orig = pd.DataFrame.from_dict(eta_orig, orient='index', columns=['Œ∑¬≤']).round(3)
            eta_df_orig = eta_df_orig.loc[feature_cols] if feature_cols else eta_df_orig
            fig7 = px.bar(eta_df_orig, y='Œ∑¬≤', color='Œ∑¬≤', range_color=[0, 1], text='Œ∑¬≤', title="Œ∑¬≤ (–¥–æ)")
            fig7.update_traces(texttemplate='%{text:.3f}', textposition='outside')
            st.plotly_chart(fig7, use_container_width=True)

        with col2:
            st.markdown("### üü¢ Œ∑¬≤ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)")
            eta_proc = calculate_correlation_ratio(df_processed, target_col)
            eta_df_proc = pd.DataFrame.from_dict(eta_proc, orient='index', columns=['Œ∑¬≤']).round(3)
            eta_df_proc = eta_df_proc.loc[feature_cols] if feature_cols else eta_df_proc
            fig8 = px.bar(eta_df_proc, y='Œ∑¬≤', color='Œ∑¬≤', range_color=[0, 1], text='Œ∑¬≤', title="Œ∑¬≤ (–ø–æ—Å–ª–µ)")
            fig8.update_traces(texttemplate='%{text:.3f}', textposition='outside')
            st.plotly_chart(fig8, use_container_width=True)

        # --- VIF ---
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### üìä VIF (–∏—Å—Ö–æ–¥–Ω—ã–µ)")
            if len(feature_cols) > 0:
                vif_orig = calculate_vif(df_original[feature_cols])
                fig5 = px.bar(vif_orig, x='feature', y='VIF', color='VIF',
                              color_continuous_scale=['green', 'orange', 'red'], range_color=[0, 20],
                              text='VIF', title="VIF (–¥–æ)")
                fig5.add_hline(y=10, line_dash="dash", line_color="red")
                st.plotly_chart(fig5, use_container_width=True)
            else:
                st.info("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è VIF.")

        with col2:
            st.markdown("### üìà VIF (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)")
            if len(feature_cols) > 0:
                vif_proc = calculate_vif(df_processed[feature_cols])
                fig6 = px.bar(vif_proc, x='feature', y='VIF', color='VIF',
                              color_continuous_scale=['green', 'orange', 'red'], range_color=[0, 20],
                              text='VIF', title="VIF (–ø–æ—Å–ª–µ)")
                fig6.add_hline(y=10, line_dash="dash", line_color="red")
                st.plotly_chart(fig6, use_container_width=True)
            else:
                st.info("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è VIF.")

        # --- –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–¥–µ–ª—å—Ç–∞) ---
        st.subheader("üìâ –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–ø–æ—Å–ª–µ - –¥–æ)")

        # Œî –ü–∏—Ä—Å–æ–Ω
        delta_corr = corr_proc - corr_orig
        st.markdown("### üîÑ Œî –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ (–ø–æ—Å–ª–µ - –¥–æ)")
        fig_dc = px.imshow(delta_corr, text_auto=True, color_continuous_scale='RdBu', zmin=-0.5, zmax=0.5,
                           title="Œî –ü–∏—Ä—Å–æ–Ω")
        st.plotly_chart(fig_dc, use_container_width=True)

        # Œî –°–ø–∏—Ä–º–∞–Ω
        delta_corr_s = corr_proc_s - corr_orig_s
        st.markdown("### üîÑ Œî –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–∞–Ω–∞ (–ø–æ—Å–ª–µ - –¥–æ)")
        fig_ds = px.imshow(delta_corr_s, text_auto=True, color_continuous_scale='RdBu', zmin=-0.5, zmax=0.5,
                           title="Œî –°–ø–∏—Ä–º–∞–Ω")
        st.plotly_chart(fig_ds, use_container_width=True)

        # Œî Œ∑¬≤
        eta_delta = {f: round(eta_proc.get(f, 0) - eta_orig.get(f, 0), 3) for f in feature_cols}
        eta_delta_df = pd.DataFrame.from_dict(eta_delta, orient='index', columns=['Œî Œ∑¬≤']).round(3)
        st.markdown("### üîó Œî Œ∑¬≤ (–ø–æ—Å–ª–µ - –¥–æ)")
        fig_de = px.bar(eta_delta_df, y='Œî Œ∑¬≤', color='Œî Œ∑¬≤',
                        color_continuous_scale=['red', 'white', 'green'], range_color=[-0.5, 0.5],
                        text='Œî Œ∑¬≤', title="Œî Œ∑¬≤")
        fig_de.add_hline(y=0, line_dash="dash", line_color="black")
        fig_de.update_traces(texttemplate='%{text:.3f}', textposition='outside')
        st.plotly_chart(fig_de, use_container_width=True)

        # Œî VIF
        vif_delta = vif_proc.set_index('feature').subtract(vif_orig.set_index('feature'), fill_value=0).round(
            3).reset_index()
        st.markdown("### üìâ Œî VIF (–ø–æ—Å–ª–µ - –¥–æ)")
        fig_dv = px.bar(vif_delta, x='feature', y='VIF', color='VIF',
                        color_continuous_scale=['red', 'white', 'green'], range_color=[-10, 10],
                        text='VIF', title="Œî VIF")
        fig_dv.add_hline(y=0, line_dash="dash", line_color="black")
        fig_dv.update_traces(texttemplate='%{text:.3f}', textposition='outside')
        st.plotly_chart(fig_dv, use_container_width=True)

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        st.session_state.status['vif_analyzed'] = True

    with tab4:
        st.header("–ê–Ω–∞–ª–∏–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")

        if st.session_state.processed_df is None:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–∫–ª–∞–¥–∫–µ.")
        else:
            df = st.session_state.processed_df.copy()
            feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df.columns]

            if len(feature_cols) == 0:
                st.warning("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.")
            else:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                X = df[feature_cols]

                # –ú–µ—Ç–æ–¥—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
                scalers = {
                    "StandardScaler (Z-score)": StandardScaler(),
                    "MinMaxScaler ([0,1])": MinMaxScaler(),
                    "RobustScaler (–º–µ–¥–∏–∞–Ω–∞/IQR)": RobustScaler(),
                    "–ë–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è": None
                }

                st.subheader("–í–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

                # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                stats = []
                for name, scaler in scalers.items():
                    if scaler is None:
                        X_scaled = X.copy()
                    else:
                        X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_cols)

                    for col in feature_cols:
                        row = {
                            "–ú–µ—Ç–æ–¥": name,
                            "–ü—Ä–∏–∑–Ω–∞–∫": col,
                            "–°—Ä–µ–¥–Ω–µ–µ": X_scaled[col].mean(),
                            "Std": X_scaled[col].std() if scaler is not None else X[col].std(),
                            "Min": X_scaled[col].min(),
                            "Max": X_scaled[col].max(),
                            "IQR": X_scaled[col].quantile(0.75) - X_scaled[col].quantile(0.25)
                        }
                        stats.append(row)

                stats_df = pd.DataFrame(stats)

                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–µ—Ç–æ–¥—É
                method_stats = stats_df.groupby("–ú–µ—Ç–æ–¥").agg(
                    –°—Ä–µ–¥–Ω–µ–µ_—Å—Ä–µ–¥–Ω–∏—Ö=("–°—Ä–µ–¥–Ω–µ–µ", "mean"),
                    –°—Ä–µ–¥–Ω–µ–µ_std=("Std", "mean"),
                    –°—Ä–µ–¥–Ω–∏–π_IQR=("IQR", "mean")
                ).round(3)

                st.dataframe(method_stats)

                # --- –í–ª–∏—è–Ω–∏–µ –Ω–∞ VIF ---
                st.subheader("–í–ª–∏—è–Ω–∏–µ –Ω–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å (VIF)")

                st.markdown("""
                **üìå –í–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ VIF**:
                - –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ **–Ω–µ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç** –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å, —Ç–∞–∫ –∫–∞–∫ VIF –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, –∞ –Ω–µ –æ—Ç –∏—Ö –º–∞—Å—à—Ç–∞–±–∞.
                - –û–¥–Ω–∞–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —Ä–∞–∑–Ω—ã–º –º–∞—Å—à—Ç–∞–±–æ–º –º–æ–≥—É—Ç –∏—Å–∫–∞–∂–∞—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –≤ –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö.
                - `RobustScaler` –∏ `StandardScaler` –ø–æ–º–æ–≥–∞—é—Ç —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ –Ω–µ –≤–ª–∏—è—é—Ç –Ω–∞ VIF –Ω–∞–ø—Ä—è–º—É—é.
                """)

                col1, col2, col3, col4 = st.columns(4)
                for i, (name, scaler) in enumerate(scalers.items()):
                    with [col1, col2, col3, col4][i]:
                        if scaler is None:
                            X_scaled = X.copy()
                        else:
                            X_scaled = scaler.fit_transform(X)
                            X_scaled = pd.DataFrame(X_scaled, columns=feature_cols)

                        vif_data = calculate_vif(X_scaled)
                        avg_vif = vif_data['VIF'].mean()
                        max_vif = vif_data['VIF'].max()

                        st.markdown(f"### {name}")
                        st.metric("–°—Ä–µ–¥–Ω–∏–π VIF", f"{avg_vif:.2f}")
                        st.metric("–ú–∞–∫—Å. VIF", f"{max_vif:.2f}")

                # --- –í–ª–∏—è–Ω–∏–µ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã ---
                st.subheader("–í–ª–∏—è–Ω–∏–µ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã (IQR)")
                outlier_summary = []
                for name, scaler in scalers.items():
                    if scaler is None:
                        X_scaled = X.copy()
                    else:
                        X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_cols)

                    total_outliers = 0
                    for col in feature_cols:
                        Q1 = X_scaled[col].quantile(0.25)
                        Q3 = X_scaled[col].quantile(0.75)
                        IQR = Q3 - Q1
                        lower = Q1 - 1.5 * IQR
                        upper = Q3 + 1.5 * IQR
                        total_outliers += ((X_scaled[col] < lower) | (X_scaled[col] > upper)).sum()

                    outlier_summary.append({"–ú–µ—Ç–æ–¥": name, "–í—ã–±—Ä–æ—Å–æ–≤": total_outliers})

                outlier_df = pd.DataFrame(outlier_summary)
                fig_outliers = px.bar(outlier_df, x="–ú–µ—Ç–æ–¥", y="–í—ã–±—Ä–æ—Å–æ–≤", color="–í—ã–±—Ä–æ—Å–æ–≤",
                                      title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
                st.plotly_chart(fig_outliers, use_container_width=True)

                # --- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è ---
                st.subheader("üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –º–µ—Ç–æ–¥—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")

                # –õ–æ–≥–∏–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                robust_outliers = outlier_df.set_index("–ú–µ—Ç–æ–¥").loc["RobustScaler (–º–µ–¥–∏–∞–Ω–∞/IQR)", "–í—ã–±—Ä–æ—Å–æ–≤"]
                std_outliers = outlier_df.set_index("–ú–µ—Ç–æ–¥").loc["StandardScaler (Z-score)", "–í—ã–±—Ä–æ—Å–æ–≤"]
                minmax_outliers = outlier_df.set_index("–ú–µ—Ç–æ–¥").loc["MinMaxScaler ([0,1])", "–í—ã–±—Ä–æ—Å–æ–≤"]

                robust_vif = vif_data[vif_data['feature'] == feature_cols[0]].iloc[0]['VIF']  # –ø—Ä–∏–º–µ—Ä
                high_vif = (vif_data['VIF'] > 10).sum() > 0

                if robust_outliers < minmax_outliers and robust_outliers < std_outliers:
                    st.success("‚úÖ **–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ: `RobustScaler`**")
                    st.write("‚Ä¢ –ù–∞–∏–ª—É—á—à–µ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤.")
                    st.write("‚Ä¢ –£—Å—Ç–æ–π—á–∏–≤ –∫ –∞–Ω–æ–º–∞–ª–∏—è–º.")
                elif minmax_outliers <= std_outliers:
                    st.info("üü° **–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ: `MinMaxScaler`**")
                    st.write("‚Ä¢ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –∏ –º–æ–¥–µ–ª–µ–π, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É.")
                    st.write("‚Ä¢ –•–æ—Ä–æ—à–æ, –µ—Å–ª–∏ –≤—ã–±—Ä–æ—Å—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã.")
                else:
                    st.info("üü° **–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ: `StandardScaler`**")
                    st.write("‚Ä¢ –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.")
                    st.write("‚Ä¢ –†–∞–±–æ—Ç–∞–µ—Ç, –µ—Å–ª–∏ –≤—ã–±—Ä–æ—Å—ã –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã.")

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–≤–µ—Ç—ã
                if high_vif:
                    st.warning(
                        "‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤—ã—Å–æ–∫–∞—è –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ç–∞–±–µ.")

                st.caption("üí° –°–æ–≤–µ—Ç: –í—ã–±–æ—Ä –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å –≤ —Ç–∞–±–µ '–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑'.")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–∞–±–∞
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ –ø–æ–ª–Ω–æ–º—É –∏–º–µ–Ω–∏
                if robust_outliers < minmax_outliers and robust_outliers < std_outliers:
                    recommended_full = "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)"
                elif minmax_outliers <= std_outliers:
                    recommended_full = "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)"
                else:
                    recommended_full = "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)"

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω–æ–µ –∏–º—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ selectbox
                st.session_state.scaler_recommendation = {
                    "recommended": recommended_full
                }

    with tab5:
        st.header("–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        if st.session_state.processed_df is not None:
            if "A" not in st.session_state.processed_df.columns:
                st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'A'")
                st.session_state.status['models_trained'] = False
            else:
                df = st.session_state.processed_df.copy()
                feature_cols = [col for col in ['B', 'C', 'D', 'E', 'F', 'G', 'H'] if col in df.columns]

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                if 'vif_remove_list' not in st.session_state:
                    st.session_state.vif_remove_list = []

                st.subheader("–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å—é (VIF ‚â• 10)")
                if len(feature_cols) > 0:
                    X_vif = df[feature_cols]
                    vif_data = calculate_vif(X_vif)
                    high_vif = vif_data[vif_data['VIF'] >= 10]

                    if not high_vif.empty:
                        st.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å—é (VIF ‚â• 10):")
                        for _, row in high_vif.iterrows():
                            checked = st.checkbox(
                                f"–£–¥–∞–ª–∏—Ç—å '{row['feature']}' (VIF = {row['VIF']:.2f})",
                                value=(row['feature'] in st.session_state.vif_remove_list),
                                key=f"vif_remove_{row['feature']}"
                            )
                            if checked and row['feature'] not in st.session_state.vif_remove_list:
                                st.session_state.vif_remove_list.append(row['feature'])
                            elif not checked and row['feature'] in st.session_state.vif_remove_list:
                                st.session_state.vif_remove_list.remove(row['feature'])
                        st.info(f"–ü–æ–º–µ—á–µ–Ω—ã –∫ —É–¥–∞–ª–µ–Ω–∏—é: {', '.join(st.session_state.vif_remove_list)}")
                    else:
                        st.success("–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (VIF < 10 –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
                        st.session_state.vif_remove_list = []
                else:
                    st.warning("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ VIF.")
                    st.session_state.vif_remove_list = []

                # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
                st.write("### –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
                st.markdown("""
                - **StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)**: –°—Ä–µ–¥–Ω–µ–µ = 0, std = 1. –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π.
                - **MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)**: –î–∏–∞–ø–∞–∑–æ–Ω [0, 1]. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π.
                - **RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)**: –£—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º.
                - **–ù–µ—Ç**: –ë–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.
                """)
                scaling_method = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è:",
                    ["–ù–µ—Ç", "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)", "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)",
                     "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)"],
                    index=["–ù–µ—Ç", "StandardScaler (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)", "MinMaxScaler (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)",
                           "RobustScaler (—É—Å—Ç–æ–π—á–∏–≤—ã–π)"]
                    .index(st.session_state.get('scaler_recommendation', {}).get('recommended', 'StandardScaler')),
                    key="scaling_method_regression"
                )

                # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
                if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏", key="run_regressions"):
                    with st.spinner("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π..."):
                        # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                        if st.session_state.vif_remove_list:
                            available_to_remove = [f for f in st.session_state.vif_remove_list if f in df.columns]
                            if available_to_remove:
                                df = df.drop(columns=available_to_remove)
                                st.success(f"–£–¥–∞–ª–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(available_to_remove)}")
                            else:
                                st.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º VIF –Ω–µ –≤—ã–±—Ä–∞–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")
                        else:
                            st.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º VIF –Ω–µ –ø–æ–º–µ—á–µ–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π df
                        st.session_state.processed_df = df.copy()

                        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
                        results, X_train, y_train = run_all_regressions(df, scaling_method)

                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        for model_name in results:
                            if "error" not in results[model_name]:  # –¢–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ –º–æ–¥–µ–ª–∏
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
                                results[model_name]["scaling_method"] = scaling_method

                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–∫—Ç scaler, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å (–¥–ª—è –º–æ–¥–µ–ª–µ–π, –∫—Ä–æ–º–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏)
                                # –ù–µ–π—Ä–æ—Å–µ—Ç—å –∏–º–µ–µ—Ç —Å–≤–æ–π –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π scaler, –µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å
                                if model_name != "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å" and scaling_method != "–ù–µ—Ç":
                                    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ run_all_regressions –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç scaler –≤ X_train –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
                                    # –ï—Å–ª–∏ scaler –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è, –µ–≥–æ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–¥–µ—Å—å
                                    if 'scaler' in results[model_name]:
                                        # –£–∂–µ –µ—Å—Ç—å (–µ—Å–ª–∏ run_all_regressions –µ–≥–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç)
                                        pass
                                    else:
                                        # –°–æ–∑–¥–∞—ë–º –∏ –æ–±—É—á–∞–µ–º scaler –∑–¥–µ—Å—å, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                                        scaler_obj = get_scaler_from_name(scaling_method)
                                        feature_cols = results[model_name]["original_features"]
                                        X_for_scaling = df[feature_cols]
                                        results[model_name]["scaler"] = scaler_obj.fit(X_for_scaling)

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–µ—Å—Å–∏—é
                        st.session_state.results = results
                        st.session_state.X_train = X_train
                        st.session_state.y_train = y_train
                        st.session_state.status['models_trained'] = True
                        st.rerun()

                # –ü–æ–∫–∞–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if st.session_state.results is not None:
                    show_regression_results(st.session_state.results, st.session_state.X_train,
                                            st.session_state.y_train)
                else:
                    st.session_state.status['models_trained'] = False
                    st.info("–ù–∞–∂–º–∏—Ç–µ '–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏', —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏.")

        else:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤–∫–ª–∞–¥–∫–∞—Ö.")
            st.session_state.status['models_trained'] = False

    with tab6:
        st.header("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —Å–∏–º—É–ª—è—Ü–∏—è")
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏
        if 'simulator_inputs' not in st.session_state:
            st.session_state.simulator_inputs = {}
        if 'optimization_history' not in st.session_state:
            st.session_state.optimization_history = []
        if 'optimization_result' not in st.session_state:
            st.session_state.optimization_result = None

        if st.session_state.results is None or st.session_state.processed_df is None:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑'.")
        else:
            valid_results = {k: v for k, v in st.session_state.results.items() if "error" not in v}
            if not valid_results:
                st.warning("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.")
            else:
                # --- –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ ---
                model_options = list(valid_results.keys())
                if 'selected_opt_model' not in st.session_state:
                    st.session_state.selected_opt_model = model_options[0]
                st.session_state.selected_opt_model = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
                    model_options,
                    index=model_options.index(st.session_state.selected_opt_model)
                )
                best_model_name = st.session_state.selected_opt_model
                best_result = valid_results[best_model_name]
                model = best_result["model"]
                feature_cols = best_result["original_features"]
                df = st.session_state.processed_df
                st.success(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: **{best_model_name}** (R¬≤ = {best_result['metrics']['r2']:.4f})")

                # --- –†–µ–∂–∏–º: –°–∏–º—É–ª—è—Ç–æ—Ä ---
                st.subheader("üéÆ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å–∏–º—É–ª—è—Ç–æ—Ä")
                st.caption(
                    "–ó–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. "
                    "–≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞."
                )
                cols = st.columns(len(feature_cols))
                inputs = {}
                for i, col in enumerate(feature_cols):
                    with cols[i % len(cols)]:
                        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ —Å–µ—Å—Å–∏–∏
                        if col not in st.session_state.simulator_inputs:
                            st.session_state.simulator_inputs[col] = float(df[col].median())
                        value = st.number_input(
                            f"{col}",
                            value=st.session_state.simulator_inputs[col],
                            min_value=float(df[col].min()),
                            max_value=float(df[col].max()),
                            step=0.01,
                            help=f"–î–æ–ø—É—Å—Ç–∏–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: –æ—Ç {df[col].min():.3f} –¥–æ {df[col].max():.3f} (–Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö)",
                            key=f"sim_input_{col}"
                        )
                        st.session_state.simulator_inputs[col] = value
                        inputs[col] = value

                X_input = pd.DataFrame([inputs])
                try:
                    pred = predict_with_model(model, best_model_name, X_input, best_result)
                    st.metric("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A", f"{pred:.4f}")
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")

                st.markdown("---")

                # --- –†–µ–∂–∏–º: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ---
                st.subheader("‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (scipy.optimize)")
                optimization_mode = st.radio(
                    "–¶–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
                    ["–ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å A", "–ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å A", "–î–æ—Å—Ç–∏—á—å —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"],
                    key="opt_mode"
                )
                target_value = None
                if optimization_mode == "–î–æ—Å—Ç–∏—á—å —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è":
                    target_value = st.number_input(
                        "–¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ A",
                        value=float(df["A"].mean()),
                        key="target_value_input"
                    )

                # –î–∏–∞–ø–∞–∑–æ–Ω—ã
                st.write("### –î–∏–∞–ø–∞–∑–æ–Ω—ã —Ñ–∞–∫—Ç–æ—Ä–æ–≤")
                bounds = []
                cols = st.columns(len(feature_cols))
                for i, col in enumerate(feature_cols):
                    with cols[i % len(cols)]:
                        low_key = f"bound_low_{col}"
                        high_key = f"bound_high_{col}"
                        if low_key not in st.session_state:
                            st.session_state[low_key] = float(df[col].min())
                        if high_key not in st.session_state:
                            st.session_state[high_key] = float(df[col].max())
                        low = st.number_input(f"–ú–∏–Ω {col}", value=st.session_state[low_key], step=0.01, key=low_key)
                        high = st.number_input(f"–ú–∞–∫—Å {col}", value=st.session_state[high_key], step=0.01, key=high_key)
                        bounds.append((low, high))

                # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
                if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", key="optimize_scipy"):
                    from scipy.optimize import minimize
                    # –ò—Å—Ç–æ—Ä–∏—è –≤—ã–∑–æ–≤–æ–≤
                    history = []

                    def predict_func(x):
                        x_clipped = np.clip(x, [b[0] for b in bounds], [b[1] for b in bounds])
                        X_point = pd.DataFrame([x_clipped], columns=feature_cols)
                        pred = predict_with_model(model, best_model_name, X_point, best_result)
                        history.append((x_clipped.copy(), pred))
                        return pred

                    def obj_func(x):
                        pred = predict_func(x)
                        if optimization_mode == "–ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å A":
                            return -pred
                        elif optimization_mode == "–ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å A":
                            return pred
                        else:
                            return (pred - target_value) ** 2

                    # –ó–∞–ø—É—Å–∫
                    result = minimize(
                        obj_func,
                        x0=[df[col].median() for col in feature_cols],
                        bounds=bounds,
                        method='L-BFGS-B'
                    )

                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏—é
                    st.session_state.optimization_history = history
                    st.session_state.optimization_result = {
                        "success": result.success,
                        "message": result.message,
                        "optimal_x": result.x,
                        "optimal_y": predict_func(result.x),
                        "mode": optimization_mode,
                        "target": target_value
                    }

                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if st.session_state.optimization_result:
                    res = st.session_state.optimization_result
                    if res["success"]:
                        st.success("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                        if res["mode"] == "–î–æ—Å—Ç–∏—á—å —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è":
                            st.write(f"**–¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {res['target']}")
                            st.write(f"**–î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–µ A:** {res['optimal_y']:.4f}")
                            st.write(f"**–û—à–∏–±–∫–∞:** {abs(res['optimal_y'] - res['target']):.4f}")
                        else:
                            st.write(f"**–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ A:** {res['optimal_y']:.4f}")
                        st.write("–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
                        for col, val in zip(feature_cols, res["optimal_x"]):
                            st.write(f"- **{col}:** {val:.4f}")
                    else:
                        st.error(f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {res['message']}")

                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
                if st.session_state.optimization_history:
                    st.subheader("üìä –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
                    history_df = pd.DataFrame(
                        [dict(zip(feature_cols, x)) for x, y in st.session_state.optimization_history],
                        index=range(len(st.session_state.optimization_history))
                    )
                    history_df["A_pred"] = [y for x, y in st.session_state.optimization_history]
                    history_df["step"] = history_df.index

                    # –ì—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è A
                    fig_a = px.line(
                        history_df,
                        x="step",
                        y="A_pred",
                        title="–ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ A –Ω–∞ —à–∞–≥–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
                        labels={"step": "–®–∞–≥", "A_pred": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ A"}
                    )
                    st.plotly_chart(fig_a, use_container_width=True)

                    # –ï—Å–ª–∏ 2+ —Ñ–∞–∫—Ç–æ—Ä–∞ ‚Äî –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ –¥–≤–∞
                    if len(feature_cols) >= 2:
                        fig_traj = px.scatter(
                            history_df,
                            x=feature_cols[0],
                            y=feature_cols[1],
                            size="A_pred",
                            color="A_pred",
                            hover_data=["step"],
                            title=f"–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {feature_cols[0]} vs {feature_cols[1]}",
                            labels={feature_cols[0]: feature_cols[0], feature_cols[1]: feature_cols[1]}
                        )
                        fig_traj.update_traces(marker=dict(sizemode='diameter', sizeref=0.1))
                        st.plotly_chart(fig_traj, use_container_width=True)

                    # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞
                    if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
                        st.session_state.optimization_history = []
                        st.session_state.optimization_result = None
                        st.rerun()


if __name__ == "__main__":
    main()
